import asyncio
import os
import re
from typing import List, Dict, Any
from .base import BaseScraper
from .github.get_followers_list import GitHubFollowersListScraper
from .github.scrape_profiles import GitHubProfileScraper
from playwright.async_api import async_playwright
from datetime import datetime

class GitHubTwoStageScraper(BaseScraper):
    """
    GitHubä¸¤é˜¶æ®µçˆ¬å–å™¨ - å®Œå…¨ç»Ÿä¸€çš„Profileè·å–æ¶æ„ + å¤šçº¿ç¨‹å¹¶å‘ä¼˜åŒ–

    ## å¹¶å‘ä¼˜åŒ–ç‰¹æ€§
    - **å¤šçº¿ç¨‹ç”¨æˆ·è¯¦æƒ…è·å–**ï¼šä½¿ç”¨asyncioå¹¶å‘è·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯ï¼Œé€Ÿåº¦æå‡5-10å€
    - **æ™ºèƒ½å¹¶å‘æ§åˆ¶**ï¼šä½¿ç”¨Semaphoreé™åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…è¢«GitHubé™åˆ¶
    - **ç‹¬ç«‹æµè§ˆå™¨å®ä¾‹**ï¼šæ¯ä¸ªå¹¶å‘ä»»åŠ¡ä½¿ç”¨ç‹¬ç«‹çš„browser contextï¼Œé¿å…å†²çª
    - **é”™è¯¯å¤„ç†ä¸é‡è¯•**ï¼šå¹¶å‘ç¯å¢ƒä¸‹çš„é”™è¯¯å¤„ç†å’Œè‡ªåŠ¨é‡è¯•æœºåˆ¶
    - **è¿›åº¦å®æ—¶æ›´æ–°**ï¼šæ”¯æŒå¹¶å‘ç¯å¢ƒä¸‹çš„å®æ—¶è¿›åº¦æŠ¥å‘Š

    ## ç»Ÿä¸€æ¶æ„è®¾è®¡
    - **ç¬¬ä¸€é˜¶æ®µ**ï¼šæ ¹æ®ä¸åŒç±»å‹è·å–ç”¨æˆ·ååˆ—è¡¨
      - Followers/Stargazers: ä½¿ç”¨GitHubFollowersListScraperè·å–CSVç”¨æˆ·åˆ—è¡¨
      - Forks: ç›´æ¥çˆ¬å–forksé¡µé¢è·å–ç”¨æˆ·åŸºæœ¬ä¿¡æ¯

    - **ç¬¬äºŒé˜¶æ®µ**ï¼šç»Ÿä¸€ä½¿ç”¨GitHubProfileScraper._get_user_details()è·å–æ‰€æœ‰ç±»å‹ç”¨æˆ·çš„è¯¦ç»†Profile
      - **å¹¶å‘ä¼˜åŒ–**ï¼šåŒæ—¶å¤„ç†å¤šä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯è·å–
      - å®Œå…¨æ¶ˆé™¤é‡å¤ä»£ç ï¼šfollowersã€stargazersã€forksä¸‰ç§ç±»å‹çš„Profileè·å–é€»è¾‘å®Œå…¨ç»Ÿä¸€
      - åˆ†é¡µçˆ¬å–ä¹Ÿä½¿ç”¨ç»Ÿä¸€çš„Profileè·å–æ–¹æ³•
      - æ‰€æœ‰ç”¨æˆ·è¯¦ç»†ä¿¡æ¯è·å–éƒ½é€šè¿‡åŒä¸€ä¸ªå…¥å£

    ## æ”¯æŒçš„çˆ¬å–ç±»å‹
    1. **Followersçˆ¬å–**ï¼šè·å–ç”¨æˆ·çš„å…³æ³¨è€…åˆ—è¡¨åŠå…¶è¯¦ç»†ä¿¡æ¯
    2. **Stargazersçˆ¬å–**ï¼šè·å–ä»“åº“çš„starç”¨æˆ·åˆ—è¡¨åŠå…¶è¯¦ç»†ä¿¡æ¯
    3. **Forksçˆ¬å–**ï¼šè·å–ä»“åº“çš„forkç”¨æˆ·åˆ—è¡¨åŠå…¶è¯¦ç»†ä¿¡æ¯

    ## ç»Ÿä¸€è¿”å›æ ¼å¼
    æ‰€æœ‰çˆ¬å–ç±»å‹çš„è¿”å›æ•°æ®éƒ½ç»è¿‡_normalize_user_data()æ–¹æ³•æ ‡å‡†åŒ–ï¼š
    - **ç›¸åŒçš„å­—æ®µç»“æ„**ï¼šusername, display_name, bio, follower_countç­‰
    - **ç›¸åŒçš„æ•°æ®ç±»å‹**ï¼šæ•°å­—å­—æ®µä¸ºintï¼Œå­—ç¬¦ä¸²å­—æ®µä¸ºstr
    - **ç»Ÿä¸€çš„å¹³å°æ ‡è¯†**ï¼šplatform='github'
    - **æ˜ç¡®çš„ç”¨æˆ·ç±»å‹æ ‡è¯†**ï¼štype='follower'/'stargazer'/'fork_owner'
    - **Forkç‰¹æœ‰å­—æ®µ**ï¼šfork_repo_nameã€fork_repo_urlã€original_repo

    ## å…³é”®ç»Ÿä¸€æ–¹æ³•
    - `_get_users_details_unified()`: ç»Ÿä¸€çš„æ‰¹é‡Profileè·å–ï¼ˆç”¨äºå®Œæ•´çˆ¬å–ï¼‰- **å¹¶å‘ä¼˜åŒ–**
    - `_get_page_users_details()`: ç»Ÿä¸€çš„åˆ†é¡µProfileè·å–ï¼ˆç”¨äºåˆ†é¡µçˆ¬å–ï¼‰- **å¹¶å‘ä¼˜åŒ–**
    - `GitHubProfileScraper._get_user_details()`: åº•å±‚ç»Ÿä¸€çš„å•ä¸ªç”¨æˆ·Profileè·å–

    æ‰€æœ‰ç±»å‹çš„ç”¨æˆ·Profileè·å–é€»è¾‘å·²å®Œå…¨ç»Ÿä¸€ï¼Œå®ç°äº†ä»£ç å¤ç”¨å’Œæ•°æ®ä¸€è‡´æ€§ã€‚
    """

    def __init__(self, concurrent_limit: int = 8):
        super().__init__()
        self.platform = "github"
        self.stage1_scraper = GitHubFollowersListScraper()
        self.stage2_scraper = GitHubProfileScraper()  # ç»Ÿä¸€çš„Profileè·å–å™¨
        self.concurrent_limit = concurrent_limit  # å¹¶å‘é™åˆ¶ï¼Œé»˜è®¤8ä¸ªå¹¶å‘

    def get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´çš„ISOæ ¼å¼å­—ç¬¦ä¸²"""
        return datetime.now().isoformat()

    def set_concurrent_limit(self, limit: int):
        """
        è®¾ç½®å¹¶å‘é™åˆ¶æ•°é‡

        Args:
            limit: å¹¶å‘æ•°é‡é™åˆ¶ (å»ºè®®1-20ä¹‹é—´)
        """
        if limit < 1:
            limit = 1
        elif limit > 20:
            limit = 20
            print("âš ï¸ å¹¶å‘æ•°é‡é™åˆ¶åœ¨20ä»¥å†…ï¼Œé¿å…è¢«GitHubé™åˆ¶")

        self.concurrent_limit = limit
        print(f"ğŸ“Š å¹¶å‘é™åˆ¶å·²è®¾ç½®ä¸º: {self.concurrent_limit}")

    def get_concurrent_limit(self) -> int:
        """è·å–å½“å‰å¹¶å‘é™åˆ¶æ•°é‡"""
        return self.concurrent_limit

    async def scrape_with_progress(self, url: str, max_pages: int = 5, max_users: int = 100):
        """
        æ‰§è¡Œå®Œæ•´çš„ä¸¤é˜¶æ®µçˆ¬å–æµç¨‹ï¼Œè¾¹çˆ¬è¾¹è¿”å›è¿›åº¦

        Args:
            url: GitHub URL
            max_pages: ç¬¬ä¸€é˜¶æ®µæœ€å¤§çˆ¬å–é¡µæ•°
            max_users: ç¬¬äºŒé˜¶æ®µæœ€å¤§å¤„ç†ç”¨æˆ·æ•°

        Yields:
            åŒ…å«è¿›åº¦ä¿¡æ¯çš„å­—å…¸
        """
        print(f"ğŸš€ å¼€å§‹GitHubä¸¤é˜¶æ®µæµå¼çˆ¬å–: {url}")

        # å‘é€å¼€å§‹æ¶ˆæ¯
        yield {
            'type': 'progress',
            'stage': 1,
            'message': 'åˆ†æURLå’Œå‡†å¤‡çˆ¬å–...',
            'progress': 0
        }

        # åˆ†æURLç±»å‹
        scrape_type, owner, repo = self._parse_url_type(url)
        print(f"è¯†åˆ«URLç±»å‹: {scrape_type}, owner: {owner}, repo: {repo}")

        stage1_csv = ""

        # æ ¹æ®max_usersè®¡ç®—éœ€è¦çš„é¡µæ•°ï¼ˆGitHubæ¯é¡µå¤§çº¦50ä¸ªç”¨æˆ·ï¼‰
        calculated_pages = max(1, min(max_pages, (max_users + 49) // 50))
        print(f"æ ¹æ®max_users={max_users}ï¼Œè®¡ç®—éœ€è¦çˆ¬å– {calculated_pages} é¡µ")

        yield {
            'type': 'progress',
            'stage': 1,
            'message': f'å‡†å¤‡çˆ¬å– {calculated_pages} é¡µç”¨æˆ·åˆ—è¡¨...',
            'progress': 5
        }

        if scrape_type == "forks":
            print(f"è¯†åˆ«ä¸ºä»“åº“forksé¡µé¢: {owner}/{repo}")

            yield {
                'type': 'progress',
                'stage': 1,
                'message': f'æ­£åœ¨çˆ¬å–ä»“åº“ {owner}/{repo} çš„forks...',
                'progress': 10
            }

            # ç›´æ¥ä½¿ç”¨å†…ç½®çš„forksçˆ¬å–æ–¹æ³•
            fork_users = await self._scrape_forks_users(url, owner, repo, max_users)

            if not fork_users:
                yield {
                    'type': 'error',
                    'message': 'ç¬¬ä¸€é˜¶æ®µå¤±è´¥ï¼Œæ²¡æœ‰æ‰¾åˆ°ä»»ä½•forkç”¨æˆ·'
                }
                return

            # é™åˆ¶ç”¨æˆ·æ•°é‡
            fork_users = fork_users[:max_users]

            yield {
                'type': 'progress',
                'stage': 1,
                'message': f'ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼Œæ‰¾åˆ° {len(fork_users)} ä¸ªforkç”¨æˆ·',
                'progress': 50
            }

            # ç¬¬äºŒé˜¶æ®µï¼šè·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯
            yield {
                'type': 'progress',
                'stage': 2,
                'message': f'å¼€å§‹ç¬¬äºŒé˜¶æ®µï¼šè·å– {len(fork_users)} ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯...',
                'progress': 60
            }

            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼å¹¶è·å–è¯¦ç»†ä¿¡æ¯
            fork_users_data = []
            for fork_user in fork_users:
                user_data = {
                    'username': fork_user['username'],
                    'type': 'fork_owner',
                    'source_user': owner,
                    'source_repo': repo,
                    'page_number': '1',
                    'scraped_at': self.get_current_time(),
                    # Forkç‰¹æœ‰ä¿¡æ¯
                    'fork_repo_name': fork_user.get('fork_repo_name', ''),
                    'fork_repo_url': fork_user.get('fork_repo_url', ''),
                    'original_repo': f"{owner}/{repo}"
                }
                fork_users_data.append(user_data)

            detailed_users = await self._get_users_details_unified(fork_users_data, 'fork_owner')

            # ç»Ÿä¸€æ ¼å¼åŒ–forkç”¨æˆ·æ•°æ®
            normalized_data = [self._normalize_user_data(user, 'fork_owner') for user in detailed_users]

            yield {
                'type': 'complete',
                'data': normalized_data,
                'total': len(normalized_data),
                'message': f'çˆ¬å–å®Œæˆï¼å…±è·å– {len(normalized_data)} ä¸ªforkç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯',
                'progress': 100,
                'platform': 'github'
            }
            return

        elif scrape_type == "repo" or scrape_type == "stargazers":
            print(f"è¯†åˆ«ä¸ºä»“åº“stargazersé¡µé¢: {owner}/{repo}")

            yield {
                'type': 'progress',
                'stage': 1,
                'message': f'æ­£åœ¨çˆ¬å–ä»“åº“ {owner}/{repo} çš„stargazers...',
                'progress': 10
            }

            # ç¬¬ä¸€é˜¶æ®µï¼šè·å–stargazersåˆ—è¡¨
            stage1_csv = await self.stage1_scraper.scrape_stargazers_list(owner, repo, calculated_pages)

        elif scrape_type == "user" or scrape_type == "followers":
            print(f"è¯†åˆ«ä¸ºç”¨æˆ·followersé¡µé¢: {owner}")

            yield {
                'type': 'progress',
                'stage': 1,
                'message': f'æ­£åœ¨çˆ¬å–ç”¨æˆ· {owner} çš„followers...',
                'progress': 10
            }

            # ç¬¬ä¸€é˜¶æ®µï¼šè·å–followersåˆ—è¡¨
            stage1_csv = await self.stage1_scraper.scrape_followers_list(owner, calculated_pages)

        else:
            yield {
                'type': 'error',
                'message': f'æ— æ³•è¯†åˆ«URLç±»å‹: {url}'
            }
            return

        if not stage1_csv or not os.path.exists(stage1_csv):
            yield {
                'type': 'error',
                'message': 'ç¬¬ä¸€é˜¶æ®µå¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆç”¨æˆ·åˆ—è¡¨æ–‡ä»¶'
            }
            return

        yield {
            'type': 'progress',
            'stage': 1,
            'message': f'ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼Œç”Ÿæˆæ–‡ä»¶: {os.path.basename(stage1_csv)}',
            'progress': 50
        }

        # ç¬¬äºŒé˜¶æ®µï¼šè·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯
        yield {
            'type': 'progress',
            'stage': 2,
            'message': f'å¼€å§‹ç¬¬äºŒé˜¶æ®µï¼šè·å–æœ€å¤š {max_users} ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯...',
            'progress': 60
        }

        # ä½¿ç”¨å¹¶å‘ä¼˜åŒ–çš„ç¬¬äºŒé˜¶æ®µçˆ¬å–
        users_data = []
        import csv

        # è¯»å–CSVæ–‡ä»¶å¹¶å‡†å¤‡ç”¨æˆ·æ•°æ®
        try:
            with open(stage1_csv, 'r', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    if len(users_data) >= max_users:
                        break
                    users_data.append(dict(row))
        except Exception as e:
            yield {
                'type': 'error',
                'message': f'è¯»å–ç”¨æˆ·åˆ—è¡¨æ–‡ä»¶å¤±è´¥: {e}'
            }
            return

        # ç¡®å®šç”¨æˆ·ç±»å‹
        user_type = 'follower' if scrape_type in ['user', 'followers'] else 'stargazer'

        yield {
            'type': 'progress',
            'stage': 2,
            'message': f'å‡†å¤‡å¹¶å‘è·å– {len(users_data)} ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯...',
            'progress': 65
        }

        # ä½¿ç”¨å¹¶å‘æ–¹æ³•è·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯
        detailed_users = await self._get_users_details_unified_with_progress(users_data, user_type,
                                                                           start_progress=70, end_progress=95)

        # é€šè¿‡å¼‚æ­¥ç”Ÿæˆå™¨è¿”å›è¿›åº¦
        async for progress_update in detailed_users:
            yield progress_update

        # è¯»å–æœ€ç»ˆç»“æœ
        yield {
            'type': 'progress',
            'stage': 2,
            'message': 'è¯»å–æœ€ç»ˆç»“æœ...',
            'progress': 95
        }

        final_data = await self._read_enriched_data(stage1_csv.replace('_raw.csv', '_enriched.csv'))

        # ç»Ÿä¸€æ ¼å¼åŒ–æ•°æ®
        normalized_data = [self._normalize_user_data(user) for user in final_data]

        yield {
            'type': 'complete',
            'data': normalized_data,
            'total': len(normalized_data),
            'message': f'çˆ¬å–å®Œæˆï¼å…±è·å– {len(normalized_data)} ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯',
            'progress': 100,
            'platform': 'github'
        }

    async def scrape(self, url: str, max_pages: int = 5, max_users: int = 100) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œå®Œæ•´çš„ä¸¤é˜¶æ®µçˆ¬å–æµç¨‹

        Args:
            url: GitHub URL
            max_pages: ç¬¬ä¸€é˜¶æ®µæœ€å¤§çˆ¬å–é¡µæ•°
            max_users: ç¬¬äºŒé˜¶æ®µæœ€å¤§å¤„ç†ç”¨æˆ·æ•°

        Returns:
            åŒ…å«è¯¦ç»†ä¿¡æ¯çš„ç”¨æˆ·åˆ—è¡¨
        """
        print(f"ğŸš€ å¼€å§‹GitHubä¸¤é˜¶æ®µçˆ¬å–: {url}")

        # åˆ†æURLç±»å‹
        scrape_type, owner, repo = self._parse_url_type(url)
        print(f"è¯†åˆ«URLç±»å‹: {scrape_type}, owner: {owner}, repo: {repo}")

        stage1_csv = ""

        # æ ¹æ®max_usersè®¡ç®—éœ€è¦çš„é¡µæ•°ï¼ˆGitHubæ¯é¡µå¤§çº¦50ä¸ªç”¨æˆ·ï¼‰
        calculated_pages = max(1, min(max_pages, (max_users + 49) // 50))  # å‘ä¸Šå–æ•´ï¼Œä½†ä¸è¶…è¿‡max_pages
        print(f"æ ¹æ®max_users={max_users}ï¼Œè®¡ç®—éœ€è¦çˆ¬å– {calculated_pages} é¡µ")

        if scrape_type == "forks":
            print(f"è¯†åˆ«ä¸ºä»“åº“forksé¡µé¢: {owner}/{repo}")

            # ç›´æ¥ä½¿ç”¨å†…ç½®çš„forksçˆ¬å–æ–¹æ³•
            fork_users = await self._scrape_forks_users(url, owner, repo, max_users)

            if not fork_users:
                print("ç¬¬ä¸€é˜¶æ®µå¤±è´¥ï¼Œæ²¡æœ‰æ‰¾åˆ°ä»»ä½•forkç”¨æˆ·")
                return []

            # é™åˆ¶ç”¨æˆ·æ•°é‡
            fork_users = fork_users[:max_users]
            print(f"ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼Œæ‰¾åˆ° {len(fork_users)} ä¸ªforkç”¨æˆ·")

            # ç¬¬äºŒé˜¶æ®µï¼šè·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯ï¼ˆä½¿ç”¨ç»Ÿä¸€æ–¹æ³•ï¼‰
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            fork_users_data = []
            for fork_user in fork_users:
                user_data = {
                    'username': fork_user['username'],
                    'type': 'fork_owner',
                    'source_user': owner,
                    'source_repo': repo,
                    'page_number': '1',
                    'scraped_at': self.get_current_time(),
                    # Forkç‰¹æœ‰ä¿¡æ¯
                    'fork_repo_name': fork_user.get('fork_repo_name', ''),
                    'fork_repo_url': fork_user.get('fork_repo_url', ''),
                    'original_repo': f"{owner}/{repo}"
                }
                fork_users_data.append(user_data)

            detailed_users = await self._get_users_details_unified(fork_users_data, 'fork_owner')
            print(f"ç¬¬äºŒé˜¶æ®µå®Œæˆï¼Œè·å–åˆ° {len(detailed_users)} ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯")

            # ç»Ÿä¸€æ ¼å¼åŒ–forkç”¨æˆ·æ•°æ®
            return [self._normalize_user_data(user, 'fork_owner') for user in detailed_users]

        elif scrape_type == "repo" or scrape_type == "stargazers":
            print(f"è¯†åˆ«ä¸ºä»“åº“stargazersé¡µé¢: {owner}/{repo}")

            # ç¬¬ä¸€é˜¶æ®µï¼šè·å–stargazersåˆ—è¡¨
            stage1_csv = await self.stage1_scraper.scrape_stargazers_list(owner, repo, calculated_pages)

        elif scrape_type == "user" or scrape_type == "followers":
            print(f"è¯†åˆ«ä¸ºç”¨æˆ·followersé¡µé¢: {owner}")

            # ç¬¬ä¸€é˜¶æ®µï¼šè·å–followersåˆ—è¡¨
            stage1_csv = await self.stage1_scraper.scrape_followers_list(owner, calculated_pages)

        else:
            print("æ— æ³•è¯†åˆ«URLç±»å‹")
            return []

        if not stage1_csv or not os.path.exists(stage1_csv):
            print("ç¬¬ä¸€é˜¶æ®µå¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆç”¨æˆ·åˆ—è¡¨æ–‡ä»¶")
            return []

        print(f"ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼Œç”Ÿæˆæ–‡ä»¶: {stage1_csv}")

        # ç¬¬äºŒé˜¶æ®µï¼šè·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯ï¼ˆç»Ÿä¸€ä½¿ç”¨GitHubProfileScraperï¼‰
        print("ğŸ” å¼€å§‹ç¬¬äºŒé˜¶æ®µï¼šè·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯...")
        stage2_csv = await self.stage2_scraper.scrape_profiles_from_csv(
            stage1_csv,
            max_users=max_users,
            batch_size=5  # å°æ‰¹æ¬¡å¤„ç†ï¼Œé¿å…è¿‡è½½
        )

        if not stage2_csv or not os.path.exists(stage2_csv):
            print("ç¬¬äºŒé˜¶æ®µå¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆè¯¦ç»†ä¿¡æ¯æ–‡ä»¶")
            return []

        print(f"ç¬¬äºŒé˜¶æ®µå®Œæˆï¼Œç”Ÿæˆæ–‡ä»¶: {stage2_csv}")

        # è¯»å–æœ€ç»ˆç»“æœ
        final_data = await self._read_enriched_data(stage2_csv)

        # ç»Ÿä¸€æ ¼å¼åŒ–æ•°æ®
        return [self._normalize_user_data(user) for user in final_data]

    async def _read_enriched_data(self, csv_file_path: str) -> List[Dict[str, Any]]:
        """è¯»å–è¯¦ç»†ä¿¡æ¯CSVæ–‡ä»¶å¹¶è¿”å›åŸå§‹æ•°æ®ï¼ˆæ ¼å¼åŒ–å°†åœ¨å¤–éƒ¨è¿›è¡Œï¼‰"""
        import csv

        users = []

        try:
            with open(csv_file_path, 'r', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    # ç›´æ¥è¯»å–åŸå§‹æ•°æ®ï¼Œä¸åšæ ¼å¼åŒ–å¤„ç†
                    users.append(dict(row))

            print(f"æˆåŠŸè¯»å– {len(users)} ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯")
            return users

        except Exception as e:
            print(f"è¯»å–è¯¦ç»†ä¿¡æ¯æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return []

    def _safe_int(self, value: str) -> int:
        """å®‰å…¨è½¬æ¢å­—ç¬¦ä¸²ä¸ºæ•´æ•°"""
        try:
            return int(value) if value else 0
        except:
            return 0

    def _normalize_user_data(self, user_data: Dict[str, Any], user_type: str = None) -> Dict[str, Any]:
        """
        ç»Ÿä¸€ç”¨æˆ·æ•°æ®æ ¼å¼ï¼Œç¡®ä¿æ‰€æœ‰ç±»å‹çš„çˆ¬å–ç»“æœéƒ½æœ‰ç›¸åŒçš„å­—æ®µç»“æ„

        Args:
            user_data: åŸå§‹ç”¨æˆ·æ•°æ®
            user_type: ç”¨æˆ·ç±»å‹ ('follower', 'stargazer', 'fork_owner')

        Returns:
            æ ‡å‡†åŒ–åçš„ç”¨æˆ·æ•°æ®
        """
        # åŸºç¡€å­—æ®µï¼ˆæ‰€æœ‰ç±»å‹éƒ½æœ‰ï¼‰
        normalized = {
            'username': user_data.get('username', ''),
            'display_name': user_data.get('display_name', user_data.get('username', '')),
            'bio': user_data.get('bio', ''),
            'avatar_url': user_data.get('avatar_url', f"https://github.com/{user_data.get('username', '')}.png"),
            'profile_url': user_data.get('profile_url', f"https://github.com/{user_data.get('username', '')}"),
                'platform': 'github',
            'type': user_type or user_data.get('type', 'user'),

            # ç¤¾äº¤ä¿¡æ¯
            'follower_count': self._safe_int(str(user_data.get('follower_count', 0))),
            'following_count': self._safe_int(str(user_data.get('following_count', 0))),
            'public_repos': self._safe_int(str(user_data.get('public_repos', 0))),

            # ä¸ªäººä¿¡æ¯
            'company': user_data.get('company', ''),
            'location': user_data.get('location', ''),
            'website': user_data.get('website', ''),
            'twitter': user_data.get('twitter', ''),
            'email': user_data.get('email', ''),

            # å…ƒæ•°æ®
            'scraped_at': user_data.get('scraped_at', user_data.get('profile_scraped_at', self.get_current_time())),
            'source_user': user_data.get('source_user', ''),
            'source_repo': user_data.get('source_repo', ''),
            'page_number': user_data.get('page_number', ''),
        }

        # Forkç‰¹æœ‰å­—æ®µ
        if user_type == 'fork_owner' or user_data.get('type') == 'fork_owner':
            normalized.update({
                'fork_repo_name': user_data.get('fork_repo_name', ''),
                'fork_repo_url': user_data.get('fork_repo_url', ''),
                'original_repo': user_data.get('original_repo', ''),
            })

        # ç”Ÿæˆadditional_info
        info_parts = []
        if normalized['source_user'] or normalized['source_repo']:
            info_parts.append(f"Source: {normalized['source_user']}/{normalized['source_repo']}")
        if normalized['page_number']:
            info_parts.append(f"Page: {normalized['page_number']}")
        if normalized.get('original_repo'):
            info_parts.append(f"Original: {normalized['original_repo']}")

        normalized['additional_info'] = ', '.join(info_parts) if info_parts else ''

        return normalized

    def _parse_url_type(self, url: str) -> tuple:
        """
        è§£æURLç±»å‹ï¼Œæ”¯æŒfollowersã€stargazersã€forks

        Returns:
            (scrape_type, owner, repo)
        """
        url = url.rstrip('/')

        # æ£€æŸ¥æ˜¯å¦æ˜¯forksé¡µé¢
        if '/network/members' in url:
            # https://github.com/owner/repo/network/members
            pattern = r'https://github\.com/([^/]+)/([^/]+)/network/members'
            match = re.match(pattern, url)
            if match:
                owner, repo = match.groups()
                return "forks", owner, repo

        # æ£€æŸ¥æ˜¯å¦åŒ…å«forkså…³é”®è¯
        if 'forks' in url or 'network' in url:
            pattern = r'https://github\.com/([^/]+)/([^/]+)'
            match = re.match(pattern, url)
            if match:
                owner, repo = match.groups()
                return "forks", owner, repo

        # æ£€æŸ¥stargazers
        if '/stargazers' in url or 'tab=stargazers' in url:
            pattern = r'https://github\.com/([^/]+)/([^/]+)'
            match = re.match(pattern, url)
            if match:
                owner, repo = match.groups()
                return "stargazers", owner, repo

        # æ£€æŸ¥followers
        if 'tab=followers' in url or '/followers' in url:
            pattern = r'https://github.com/([^/]+)'
            match = re.match(pattern, url)
            if match:
                owner = match.group(1)
                return "followers", owner, ""

        # è§£æåŸºæœ¬URLç»“æ„
        url_parts = url.replace('https://github.com/', '').split('/')

        if len(url_parts) >= 2:
            # https://github.com/owner/repo - é»˜è®¤ä¸ºstargazers
            owner, repo = url_parts[0], url_parts[1]
            return "repo", owner, repo
        elif len(url_parts) == 1:
            # https://github.com/username - é»˜è®¤ä¸ºfollowers
            owner = url_parts[0]
            return "user", owner, ""

        raise ValueError(f"æ— æ³•è§£æURL: {url}")

    def _normalize_forks_url(self, url: str) -> str:
        """è§„èŒƒåŒ–URLï¼Œç¡®ä¿æŒ‡å‘/network/membersé¡µé¢"""
        url = url.rstrip('/')

        # å¦‚æœå·²ç»æ˜¯network/membersé¡µé¢ï¼Œç›´æ¥è¿”å›
        if '/network/members' in url:
            return url

        # å¦‚æœæ˜¯åŸºæœ¬çš„ä»“åº“URLï¼Œè½¬æ¢ä¸ºnetwork/members
        pattern = r'https://github\.com/([^/]+)/([^/]+)'
        match = re.match(pattern, url)
        if match:
            owner, repo = match.groups()
            return f"https://github.com/{owner}/{repo}/network/members"

        return None

    async def _scrape_forks_users(self, url: str, owner: str, repo: str, max_users: int = 100) -> List[Dict[str, str]]:
        """
        é˜¶æ®µ1ï¼šçˆ¬å–GitHub forksé¡µé¢ï¼Œè·å–æ‰€æœ‰forkç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯
        """
        print(f"å¼€å§‹çˆ¬å–forksé¡µé¢: {url}")

        # è§„èŒƒåŒ–URL
        normalized_url = self._normalize_forks_url(url)
        if not normalized_url:
            print("æ— æ³•è§„èŒƒåŒ–forks URL")
            return []

        await self.setup_browser()

        try:
            await self.page.goto(normalized_url, wait_until="networkidle", timeout=30000)
            await asyncio.sleep(3)

            # æ£€æŸ¥é¡µé¢æ˜¯å¦æ­£ç¡®åŠ è½½
            page_title = await self.page.title()
            print(f"é¡µé¢æ ‡é¢˜: {page_title}")

            # è‡ªåŠ¨æ»šåŠ¨åŠ è½½æ›´å¤šfork
            print("æ­£åœ¨æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šfork...")
            await self._scroll_to_load_forks()

            # æŸ¥æ‰¾forkåˆ—è¡¨
            fork_users = []
            seen_users = set()

            # ä½¿ç”¨æŒ‡å®šçš„CSSé€‰æ‹©å™¨è·å–forkç”¨æˆ·é“¾æ¥
            user_links = await self.page.query_selector_all('#network div div a:nth-child(3)')
            print(f"é€šè¿‡ '#network div div a:nth-child(3)' æ‰¾åˆ° {len(user_links)} ä¸ªç”¨æˆ·é“¾æ¥")

            for link in user_links:
                try:
                    href = await link.get_attribute('href')
                    if not href or not href.startswith('/'):
                        continue

                    # è·å–ç”¨æˆ·åï¼ˆä» /username æ ¼å¼ä¸­æå–ï¼‰
                    username = href.strip('/')

                    # è·³è¿‡åŸå§‹ä»“åº“çš„æ‰€æœ‰è€…
                    if username == owner:
                        continue

                    # é¿å…é‡å¤ç”¨æˆ·
                    if username not in seen_users:
                        seen_users.add(username)

                        # ç›´æ¥ä½¿ç”¨åŸä»“åº“åä½œä¸ºforkä»“åº“åï¼ˆé€šå¸¸forkä¿æŒç›¸åŒåç§°ï¼‰
                        fork_repo_name = repo

                        fork_user = {
                            'username': username,
                            'fork_repo_name': fork_repo_name,
                            'fork_repo_url': f"https://github.com/{username}/{fork_repo_name}",
                            'profile_url': f"https://github.com/{username}",
                            'avatar_url': f'https://github.com/{username}.png'
                        }

                        fork_users.append(fork_user)

                        if len(fork_users) >= max_users:  # é™åˆ¶æ•°é‡
                            break

                except Exception as e:
                    print(f"å¤„ç†ç”¨æˆ·é“¾æ¥æ—¶å‡ºé”™: {e}")
                    continue

            print(f"é˜¶æ®µ1å®Œæˆï¼Œæ‰¾åˆ° {len(fork_users)} ä¸ªå”¯ä¸€çš„forkç”¨æˆ·")
            return fork_users

        except Exception as e:
            print(f"çˆ¬å–forkç”¨æˆ·åˆ—è¡¨æ—¶å‡ºé”™: {e}")
            return []
        finally:
            await self.cleanup()

    async def _scroll_to_load_forks(self):
        """æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ›´å¤šfork"""
        print("å¼€å§‹æ»šåŠ¨åŠ è½½æ›´å¤šfork...")
        last_height = 0
        scroll_count = 0
        max_scrolls = 10

        while scroll_count < max_scrolls:
            # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
            await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(3)

            # æ£€æŸ¥é¡µé¢é«˜åº¦æ˜¯å¦å˜åŒ–
            new_height = await self.page.evaluate("document.body.scrollHeight")

            if new_height == last_height:
                print("é¡µé¢é«˜åº¦æœªå˜åŒ–ï¼Œåœæ­¢æ»šåŠ¨")
                break

            last_height = new_height
            scroll_count += 1
            print(f"æ»šåŠ¨æ¬¡æ•°: {scroll_count}, é¡µé¢é«˜åº¦: {new_height}")

        print(f"æ»šåŠ¨å®Œæˆï¼Œæ€»å…±æ»šåŠ¨ {scroll_count} æ¬¡")

    async def _get_single_user_concurrent(self, user_data: Dict[str, Any], user_type: str,
                                         semaphore: asyncio.Semaphore, playwright_instance) -> Dict[str, Any]:
        """
        å¹¶å‘è·å–å•ä¸ªç”¨æˆ·è¯¦ç»†ä¿¡æ¯çš„è¾…åŠ©æ–¹æ³•

        Args:
            user_data: ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
            user_type: ç”¨æˆ·ç±»å‹
            semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡
            playwright_instance: Playwrightå®ä¾‹

        Returns:
            ç”¨æˆ·è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        async with semaphore:  # é™åˆ¶å¹¶å‘æ•°é‡
            browser = None
            try:
                username = user_data.get('username', user_data.get('username'))

                # ä¸ºæ¯ä¸ªå¹¶å‘ä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„browser context
                browser = await playwright_instance.chromium.launch(headless=True)
                context = await browser.new_context()
                page = await context.new_page()

                # è®¾ç½®ç”¨æˆ·ä»£ç†
                await page.set_extra_http_headers({
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                })

                # ç¡®ä¿user_dataåŒ…å«å¿…è¦å­—æ®µ
                if 'type' not in user_data:
                    user_data['type'] = user_type
                if 'scraped_at' not in user_data:
                    user_data['scraped_at'] = self.get_current_time()

                # ä½¿ç”¨GitHubProfileScraperç»Ÿä¸€è·å–è¯¦ç»†ä¿¡æ¯
                user_info = await self.stage2_scraper._get_user_details(username, page, user_data)

                if user_info:
                    # ä¿ç•™åŸå§‹æ•°æ®ä¸­çš„ç‰¹æ®Šå­—æ®µï¼ˆå¦‚forkç‰¹æœ‰ä¿¡æ¯ï¼‰
                    for key, value in user_data.items():
                        if key not in user_info and value:
                            user_info[key] = value
                    print(f"âœ… æˆåŠŸè·å–{user_type}ç”¨æˆ· {username} çš„è¯¦ç»†ä¿¡æ¯")
                    return user_info
                else:
                    print(f"âŒ æ— æ³•è·å–{user_type}ç”¨æˆ· {username} çš„è¯¦ç»†ä¿¡æ¯")
                    return None

            except Exception as e:
                print(f"è·å–{user_type}ç”¨æˆ· {user_data.get('username', 'unknown')} è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                return None
            finally:
                if browser:
                    await browser.close()

    async def _get_users_details_unified(self, users_list: List[Dict[str, Any]], user_type: str = 'user') -> List[Dict[str, Any]]:
        """
        ç»Ÿä¸€çš„ç¬¬äºŒé˜¶æ®µï¼šè·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯ - å¹¶å‘ä¼˜åŒ–ç‰ˆæœ¬

        Args:
            users_list: ç”¨æˆ·åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«usernameç­‰åŸºæœ¬ä¿¡æ¯
            user_type: ç”¨æˆ·ç±»å‹ ('follower', 'stargazer', 'fork_owner')

        Returns:
            åŒ…å«è¯¦ç»†ä¿¡æ¯çš„ç”¨æˆ·åˆ—è¡¨
        """
        print(f"ğŸ” ä½¿ç”¨ç»Ÿä¸€Profileè·å–å™¨å¹¶å‘è·å– {len(users_list)} ä¸ª{user_type}ç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯...")
        print(f"ğŸ“Š å¹¶å‘é™åˆ¶: {self.concurrent_limit} ä¸ªä»»åŠ¡")

        # å¯åŠ¨playwright
        from playwright.async_api import async_playwright
        playwright = await async_playwright().start()

        try:
            # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°é‡
            semaphore = asyncio.Semaphore(self.concurrent_limit)

            # åˆ›å»ºæ‰€æœ‰å¹¶å‘ä»»åŠ¡
            tasks = []
            for user_data in users_list:
                task = asyncio.create_task(
                    self._get_single_user_concurrent(user_data, user_type, semaphore, playwright)
                )
                tasks.append(task)

            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ï¼Œå¹¶æ˜¾ç¤ºè¿›åº¦
            print(f"ğŸš€ å¼€å§‹å¹¶å‘æ‰§è¡Œ {len(tasks)} ä¸ªä»»åŠ¡...")
            results = []
            completed = 0

            # ä½¿ç”¨as_completedæ¥è·å–å®Œæˆçš„ä»»åŠ¡å¹¶å®æ—¶æ˜¾ç¤ºè¿›åº¦
            for coro in asyncio.as_completed(tasks):
                result = await coro
                if result:
                    results.append(result)
                completed += 1

                # æ¯å®Œæˆ10ä¸ªæˆ–å®Œæˆæ‰€æœ‰ä»»åŠ¡æ—¶æ˜¾ç¤ºè¿›åº¦
                if completed % 10 == 0 or completed == len(tasks):
                    success_rate = len(results) / completed * 100 if completed > 0 else 0
                    print(f"ğŸ“ˆ è¿›åº¦: {completed}/{len(tasks)} ({completed/len(tasks)*100:.1f}%) - æˆåŠŸç‡: {success_rate:.1f}%")

            print(f"âœ… {user_type}ç”¨æˆ·è¯¦ç»†ä¿¡æ¯è·å–å®Œæˆï¼ŒæˆåŠŸè·å– {len(results)} ä¸ªç”¨æˆ· (æˆåŠŸç‡: {len(results)/len(users_list)*100:.1f}%)")
            return results

        except Exception as e:
            print(f"è·å–{user_type}ç”¨æˆ·è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return []
        finally:
            await playwright.stop()

    async def _get_users_details_unified_with_progress(self, users_list: List[Dict[str, Any]], user_type: str = 'user',
                                                     start_progress: int = 70, end_progress: int = 95):
        """
        å¸¦è¿›åº¦æŠ¥å‘Šçš„ç»Ÿä¸€ç”¨æˆ·è¯¦ç»†ä¿¡æ¯è·å–æ–¹æ³• - å¹¶å‘ä¼˜åŒ–ç‰ˆæœ¬

        Args:
            users_list: ç”¨æˆ·åˆ—è¡¨
            user_type: ç”¨æˆ·ç±»å‹
            start_progress: å¼€å§‹è¿›åº¦å€¼
            end_progress: ç»“æŸè¿›åº¦å€¼

        Yields:
            è¿›åº¦æ›´æ–°å­—å…¸
        """
        print(f"ğŸ” ä½¿ç”¨ç»Ÿä¸€Profileè·å–å™¨å¹¶å‘è·å– {len(users_list)} ä¸ª{user_type}ç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯...")
        print(f"ğŸ“Š å¹¶å‘é™åˆ¶: {self.concurrent_limit} ä¸ªä»»åŠ¡")

        yield {
            'type': 'progress',
            'stage': 2,
            'message': f'å¯åŠ¨å¹¶å‘è·å– {len(users_list)} ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯...',
            'progress': start_progress
        }

        # å¯åŠ¨playwright
        from playwright.async_api import async_playwright
        playwright = await async_playwright().start()

        try:
            # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°é‡
            semaphore = asyncio.Semaphore(self.concurrent_limit)

            # åˆ›å»ºæ‰€æœ‰å¹¶å‘ä»»åŠ¡
            tasks = []
            for user_data in users_list:
                task = asyncio.create_task(
                    self._get_single_user_concurrent(user_data, user_type, semaphore, playwright)
                )
                tasks.append(task)

            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ï¼Œå¹¶å®æ—¶æŠ¥å‘Šè¿›åº¦
            print(f"ğŸš€ å¼€å§‹å¹¶å‘æ‰§è¡Œ {len(tasks)} ä¸ªä»»åŠ¡...")
            results = []
            completed = 0

            # ä½¿ç”¨as_completedæ¥è·å–å®Œæˆçš„ä»»åŠ¡å¹¶å®æ—¶æ˜¾ç¤ºè¿›åº¦
            for coro in asyncio.as_completed(tasks):
                result = await coro
                if result:
                    results.append(result)
                completed += 1

                # è®¡ç®—å½“å‰è¿›åº¦
                progress_ratio = completed / len(tasks)
                current_progress = start_progress + (end_progress - start_progress) * progress_ratio
                success_rate = len(results) / completed * 100 if completed > 0 else 0

                # æ¯å®Œæˆ5ä¸ªæˆ–åˆ°è¾¾ç‰¹å®šèŠ‚ç‚¹æ—¶æŠ¥å‘Šè¿›åº¦
                if completed % 5 == 0 or completed == len(tasks) or completed % (len(tasks) // 10 + 1) == 0:
                    yield {
                        'type': 'progress',
                        'stage': 2,
                        'message': f'å¹¶å‘è·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯ä¸­... ({completed}/{len(tasks)})',
                        'progress': min(end_progress, current_progress),
                        'processed_count': completed,
                        'total_count': len(tasks),
                        'success_count': len(results),
                        'success_rate': f'{success_rate:.1f}%'
                    }

            # æœ€ç»ˆç»“æœ
            final_data = [self._normalize_user_data(user) for user in results]

            yield {
                'type': 'complete',
                'data': final_data,
                'total': len(final_data),
                'message': f'å¹¶å‘çˆ¬å–å®Œæˆï¼å…±è·å– {len(final_data)} ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯ (æˆåŠŸç‡: {len(final_data)/len(users_list)*100:.1f}%)',
                'progress': 100,
                'platform': 'github'
            }

        except Exception as e:
            yield {
                'type': 'error',
                'message': f'å¹¶å‘è·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {e}'
            }
        finally:
            await playwright.stop()

    async def _get_page_single_user_concurrent(self, username: str, user_type: str, source_user: str,
                                             source_repo: str, page_number: int, semaphore: asyncio.Semaphore,
                                             playwright_instance) -> Dict[str, Any]:
        """
        åˆ†é¡µä¸­çš„å•ä¸ªç”¨æˆ·å¹¶å‘è·å–æ–¹æ³•

        Args:
            username: ç”¨æˆ·å
            user_type: ç”¨æˆ·ç±»å‹
            source_user: æºç”¨æˆ·
            source_repo: æºä»“åº“
            page_number: é¡µç 
            semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡
            playwright_instance: Playwrightå®ä¾‹

        Returns:
            ç”¨æˆ·è¯¦ç»†ä¿¡æ¯
        """
        async with semaphore:  # é™åˆ¶å¹¶å‘æ•°é‡
            browser = None
            try:
                # ä¸ºæ¯ä¸ªå¹¶å‘ä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„browser context
                browser = await playwright_instance.chromium.launch(headless=True)
                context = await browser.new_context()
                page = await context.new_page()

                # è®¾ç½®ç”¨æˆ·ä»£ç†
                await page.set_extra_http_headers({
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                })

                # æ„é€ æ ‡å‡†æ ¼å¼çš„ç”¨æˆ·æ•°æ®
                user_data = {
                    'username': username,
                    'type': user_type,
                    'source_user': source_user,
                    'source_repo': source_repo,
                    'page_number': str(page_number),
                    'scraped_at': datetime.now().isoformat()
                }

                # ä½¿ç”¨GitHubProfileScraperçš„_get_user_detailsæ–¹æ³•
                user_info = await self.stage2_scraper._get_user_details(username, page, user_data)
                if user_info:
                    return user_info
                else:
                    # è¿”å›åŸºæœ¬ä¿¡æ¯ä½œä¸ºå¤‡é€‰
                    return {
                        'username': username,
                        'display_name': username,
                        'bio': '',
                        'avatar_url': f"https://github.com/{username}.png",
                        'profile_url': f"https://github.com/{username}",
                        'platform': 'github',
                        'type': user_type,
                        'follower_count': 0,
                        'following_count': 0,
                        'company': '',
                        'location': '',
                        'website': '',
                        'twitter': '',
                        'email': '',
                        'public_repos': 0,
                        'scraped_at': datetime.now().isoformat(),
                        'source_user': source_user,
                        'source_repo': source_repo,
                        'page_number': str(page_number)
                    }

            except Exception as e:
                print(f"è·å–ç”¨æˆ· {username} è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
                # è¿”å›åŸºæœ¬ä¿¡æ¯
                return {
                    'username': username,
                    'display_name': username,
                    'bio': '',
                    'avatar_url': f"https://github.com/{username}.png",
                    'profile_url': f"https://github.com/{username}",
                    'platform': 'github',
                    'type': user_type,
                    'follower_count': 0,
                    'following_count': 0,
                    'company': '',
                    'location': '',
                    'website': '',
                    'twitter': '',
                    'email': '',
                    'public_repos': 0,
                    'scraped_at': datetime.now().isoformat(),
                    'source_user': source_user,
                    'source_repo': source_repo,
                    'page_number': str(page_number)
                }
            finally:
                if browser:
                    await browser.close()

    async def _get_page_users_details(self, usernames: List[str], page_obj, user_type: str,
                                    source_user: str, source_repo: str, page_number: int) -> List[Dict[str, Any]]:
        """
        åˆ†é¡µä¸­çš„ç»Ÿä¸€ç”¨æˆ·è¯¦ç»†ä¿¡æ¯è·å–æ–¹æ³• - å¹¶å‘ä¼˜åŒ–ç‰ˆæœ¬

        Args:
            usernames: ç”¨æˆ·ååˆ—è¡¨
            page_obj: Playwrighté¡µé¢å¯¹è±¡ (åœ¨å¹¶å‘ç‰ˆæœ¬ä¸­ä¸ä½¿ç”¨)
            user_type: ç”¨æˆ·ç±»å‹
            source_user: æºç”¨æˆ·
            source_repo: æºä»“åº“
            page_number: é¡µç 

        Returns:
            åŒ…å«è¯¦ç»†ä¿¡æ¯çš„ç”¨æˆ·åˆ—è¡¨
        """
        print(f"ğŸ” å¹¶å‘è·å–ç¬¬{page_number}é¡µ {len(usernames)} ä¸ª{user_type}ç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯...")
        print(f"ğŸ“Š å¹¶å‘é™åˆ¶: {self.concurrent_limit} ä¸ªä»»åŠ¡")

        # å¯åŠ¨playwrightå®ä¾‹
        from playwright.async_api import async_playwright
        playwright = await async_playwright().start()

        try:
            # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°é‡
            semaphore = asyncio.Semaphore(self.concurrent_limit)

            # åˆ›å»ºæ‰€æœ‰å¹¶å‘ä»»åŠ¡
            tasks = []
            for username in usernames:
                task = asyncio.create_task(
                    self._get_page_single_user_concurrent(
                        username, user_type, source_user, source_repo, page_number, semaphore, playwright
                    )
                )
                tasks.append(task)

            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            print(f"ğŸš€ å¼€å§‹å¹¶å‘æ‰§è¡Œ {len(tasks)} ä¸ªä»»åŠ¡...")
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # è¿‡æ»¤å‡ºæˆåŠŸçš„ç»“æœ
            users = []
            for result in results:
                if isinstance(result, dict):
                    users.append(result)
                else:
                    print(f"ä»»åŠ¡å¤±è´¥: {result}")

            print(f"âœ… ç¬¬{page_number}é¡µç”¨æˆ·è¯¦ç»†ä¿¡æ¯è·å–å®Œæˆï¼ŒæˆåŠŸè·å– {len(users)} ä¸ªç”¨æˆ·")
            return users

        except Exception as e:
            print(f"è·å–ç¬¬{page_number}é¡µç”¨æˆ·è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return []
        finally:
            await playwright.stop()

    async def scrape_page(self, url: str, page: int = 1) -> Dict:
        """åˆ†é¡µçˆ¬å–æ–¹æ³•"""
        try:
            print(f"GitHubåˆ†é¡µçˆ¬å–å™¨æ”¶åˆ°URL: {url}, é¡µç : {page}")

            # è§£æURLç¡®å®šçˆ¬å–ç±»å‹
            scrape_type, target_user, target_repo = self._parse_url_type(url)

            if scrape_type == "followers":
                print(f"è¯†åˆ«ä¸ºfollowersé¡µé¢ï¼Œç¬¬{page}é¡µ")
                return await self._scrape_followers_page(url, page)
            elif scrape_type == "stargazers":
                print(f"è¯†åˆ«ä¸ºstargazersé¡µé¢ï¼Œç¬¬{page}é¡µ")
                return await self._scrape_stargazers_page(url, target_user, target_repo, page)
            elif scrape_type == "forks":
                print(f"è¯†åˆ«ä¸ºforksé¡µé¢ï¼Œç¬¬{page}é¡µ")
                # forksä¸æ”¯æŒåˆ†é¡µæ¨¡å¼ï¼Œè¿”å›é”™è¯¯
                raise ValueError("Forksçˆ¬å–ä¸æ”¯æŒåˆ†é¡µæ¨¡å¼ï¼Œè¯·ä½¿ç”¨å®Œæ•´çˆ¬å–æ–¹æ³•")
            elif scrape_type == "user":
                print(f"è¯†åˆ«ä¸ºç”¨æˆ·é¡µé¢: {target_user}ï¼Œç¬¬{page}é¡µ")
                # é»˜è®¤çˆ¬å–ç”¨æˆ·çš„followers
                followers_url = f"https://github.com/{target_user}?tab=followers"
                return await self._scrape_followers_page(followers_url, page)
            elif scrape_type == "repo":
                print(f"è¯†åˆ«ä¸ºRepositoriesé¡µé¢: {target_user}/{target_repo}ï¼Œç¬¬{page}é¡µ")
                # é»˜è®¤çˆ¬å–Repositoriesçš„stargazers
                stargazers_url = f"https://github.com/{target_user}/{target_repo}/stargazers"
                return await self._scrape_stargazers_page(stargazers_url, target_user, target_repo, page)
            else:
                raise ValueError(f"æ— æ³•è¯†åˆ«çš„URLç±»å‹: {url}")

        except Exception as e:
            print(f"GitHubåˆ†é¡µçˆ¬å–å¤±è´¥: {e}")
            raise e

    async def _scrape_followers_page(self, url: str, page: int) -> Dict:
        """åˆ†é¡µçˆ¬å–followers"""
        try:
            print(f"å¼€å§‹çˆ¬å–å…³æ³¨è€…é¡µé¢ç¬¬{page}é¡µ: {url}")

            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context()
                page_obj = await context.new_page()

                try:
                    # æ„å»ºåˆ†é¡µURL
                    if '?' in url:
                        page_url = f"{url}&page={page}"
                    else:
                        page_url = f"{url}?page={page}"

                    print(f"è®¿é—®åˆ†é¡µURL: {page_url}")
                    await page_obj.goto(page_url, wait_until='networkidle', timeout=30000)

                    # ç­‰å¾…ç”¨æˆ·åˆ—è¡¨åŠ è½½
                    await page_obj.wait_for_selector('a[data-hovercard-type="user"]', timeout=10000)

                    # è·å–ç”¨æˆ·é“¾æ¥
                    user_links = await page_obj.query_selector_all('a[data-hovercard-type="user"]')
                    print(f"æ‰¾åˆ° {len(user_links)} ä¸ªç”¨æˆ·é“¾æ¥å…ƒç´ ")

                    # æå–ç”¨æˆ·ååˆ—è¡¨ï¼Œä½¿ç”¨setå»é‡
                    usernames = []
                    seen_usernames = set()
                    for link in user_links:
                        try:
                            href = await link.get_attribute('href')
                            if href and href.startswith('/'):
                                username = href.strip('/')
                                # å»é‡ï¼šå¦‚æœç”¨æˆ·åå·²ç»å­˜åœ¨ï¼Œè·³è¿‡
                                if username and username not in seen_usernames:
                                    seen_usernames.add(username)
                                    usernames.append(username)

                                    # é™åˆ¶æ¯é¡µæœ€å¤š50ä¸ªç”¨æˆ·
                                    if len(usernames) >= 50:
                                        break
                        except Exception as e:
                            print(f"æå–ç”¨æˆ·åå¤±è´¥: {e}")
                            continue

                    print(f"å¼€å§‹è·å– {len(usernames)} ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯...")

                    # ä½¿ç”¨ç»Ÿä¸€çš„Profileè·å–å™¨
                    users = await self._get_page_users_details(usernames, page_obj, 'follower', '', '', page)

                    # æŒ‰followeræ•°é‡æ’åºï¼ˆé™åºï¼‰
                    users.sort(key=lambda x: x['follower_count'], reverse=True)
                    print(f"ç”¨æˆ·æŒ‰followeræ•°é‡æ’åºå®Œæˆï¼Œæœ€é«˜: {users[0]['follower_count'] if users else 0}")

                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ - ä½¿ç”¨å¤šç§ç­–ç•¥
                    has_next_page = False
                    try:
                        # GitHubå¯èƒ½ä½¿ç”¨ä¸åŒçš„åˆ†é¡µé€‰æ‹©å™¨
                        selectors_to_check = [
                            '.pagination a[rel="next"]',
                            '.paginate-container .next_page',
                            '.paginate-container a[rel="next"]',
                            'a[aria-label="Next"]',
                            '.BtnGroup a[rel="next"]',
                            '.pagination .next_page:not(.disabled)',
                            '.paginate-container .next_page:not(.disabled)'
                        ]

                        for selector in selectors_to_check:
                            next_button = await page_obj.query_selector(selector)
                            if next_button:
                                # æ£€æŸ¥æŒ‰é’®æ˜¯å¦è¢«ç¦ç”¨
                                is_disabled = await next_button.get_attribute('aria-disabled')
                                class_name = await next_button.get_attribute('class') or ''
                                if is_disabled != 'true' and 'disabled' not in class_name:
                                    has_next_page = True
                                    print(f"æ‰¾åˆ°æœ‰æ•ˆçš„ä¸‹ä¸€é¡µæŒ‰é’®: {selector}")
                                    break

                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œæ£€æŸ¥å½“å‰é¡µé¢çš„ç”¨æˆ·æ•°é‡
                        # å¦‚æœæ­£å¥½æ˜¯50ä¸ªç”¨æˆ·ï¼Œå¾ˆå¯èƒ½è¿˜æœ‰ä¸‹ä¸€é¡µ
                        if not has_next_page and len(users) >= 50:
                            has_next_page = True
                            print(f"åŸºäºç”¨æˆ·æ•°é‡({len(users)})åˆ¤æ–­å¯èƒ½æœ‰ä¸‹ä¸€é¡µ")

                    except Exception as e:
                        print(f"æ£€æŸ¥ä¸‹ä¸€é¡µæ—¶å‡ºé”™: {e}")
                        # å¦‚æœå‡ºé”™ä¸”ç”¨æˆ·æ•°é‡è¾¾åˆ°50ï¼Œå‡è®¾æœ‰ä¸‹ä¸€é¡µ
                        if len(users) >= 50:
                            has_next_page = True

                    # ç»Ÿä¸€æ ¼å¼åŒ–æ•°æ®
                    users = [self._normalize_user_data(user, 'follower') for user in users]

                    print(f"æˆåŠŸæå–äº†ç¬¬{page}é¡µ {len(users)} ä¸ªå…³æ³¨è€…")

                    return {
                        'data': users,
                        'has_next_page': has_next_page,
                        'current_page': page
                    }

                finally:
                    await browser.close()

        except Exception as e:
            print(f"çˆ¬å–followersç¬¬{page}é¡µæ—¶å‡ºé”™: {e}")
            return {
                'data': [],
                'has_next_page': False,
                'current_page': page
            }

    async def _scrape_stargazers_page(self, url: str, owner: str, repo: str, page: int) -> Dict:
        """åˆ†é¡µçˆ¬å–stargazers"""
        try:
            print(f"å¼€å§‹çˆ¬å–stargazersé¡µé¢ç¬¬{page}é¡µ: {url}")

            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context()
                page_obj = await context.new_page()

                try:
                    # æ„å»ºåˆ†é¡µURL
                    page_url = f"{url}?page={page}"

                    print(f"è®¿é—®åˆ†é¡µURL: {page_url}")
                    await page_obj.goto(page_url, wait_until='networkidle', timeout=30000)

                    # ç­‰å¾…ç”¨æˆ·åˆ—è¡¨åŠ è½½
                    await page_obj.wait_for_selector('a[data-hovercard-type="user"]', timeout=10000)

                    # è·å–ç”¨æˆ·é“¾æ¥
                    user_links = await page_obj.query_selector_all('a[data-hovercard-type="user"]')
                    print(f"æ‰¾åˆ° {len(user_links)} ä¸ªç”¨æˆ·é“¾æ¥å…ƒç´ ")

                    # æå–ç”¨æˆ·ååˆ—è¡¨ï¼Œä½¿ç”¨setå»é‡
                    usernames = []
                    seen_usernames = set()
                    for link in user_links:
                        try:
                            href = await link.get_attribute('href')
                            if href and href.startswith('/'):
                                username = href.strip('/')
                                # å»é‡ï¼šå¦‚æœç”¨æˆ·åå·²ç»å­˜åœ¨ï¼Œè·³è¿‡
                                if username and username not in seen_usernames:
                                    seen_usernames.add(username)
                                    usernames.append(username)

                                    # é™åˆ¶æ¯é¡µæœ€å¤š50ä¸ªç”¨æˆ·
                                    if len(usernames) >= 50:
                                        break
                        except Exception as e:
                            print(f"æå–ç”¨æˆ·åå¤±è´¥: {e}")
                            continue

                    print(f"å¼€å§‹è·å– {len(usernames)} ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯...")

                    # ä½¿ç”¨ç»Ÿä¸€çš„Profileè·å–å™¨
                    users = await self._get_page_users_details(usernames, page_obj, 'stargazer', owner, repo, page)

                    # æŒ‰followeræ•°é‡æ’åºï¼ˆé™åºï¼‰
                    users.sort(key=lambda x: x['follower_count'], reverse=True)
                    print(f"ç”¨æˆ·æŒ‰followeræ•°é‡æ’åºå®Œæˆï¼Œæœ€é«˜: {users[0]['follower_count'] if users else 0}")

                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ - ä½¿ç”¨å¤šç§ç­–ç•¥
                    has_next_page = False
                    try:
                        # GitHubå¯èƒ½ä½¿ç”¨ä¸åŒçš„åˆ†é¡µé€‰æ‹©å™¨
                        selectors_to_check = [
                            '.pagination a[rel="next"]',
                            '.paginate-container .next_page',
                            '.paginate-container a[rel="next"]',
                            'a[aria-label="Next"]',
                            '.BtnGroup a[rel="next"]',
                            '.pagination .next_page:not(.disabled)',
                            '.paginate-container .next_page:not(.disabled)'
                        ]

                        for selector in selectors_to_check:
                            next_button = await page_obj.query_selector(selector)
                            if next_button:
                                # æ£€æŸ¥æŒ‰é’®æ˜¯å¦è¢«ç¦ç”¨
                                is_disabled = await next_button.get_attribute('aria-disabled')
                                class_name = await next_button.get_attribute('class') or ''
                                if is_disabled != 'true' and 'disabled' not in class_name:
                                    has_next_page = True
                                    print(f"æ‰¾åˆ°æœ‰æ•ˆçš„ä¸‹ä¸€é¡µæŒ‰é’®: {selector}")
                                    break

                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œæ£€æŸ¥å½“å‰é¡µé¢çš„ç”¨æˆ·æ•°é‡
                        # å¦‚æœæ­£å¥½æ˜¯50ä¸ªç”¨æˆ·ï¼Œå¾ˆå¯èƒ½è¿˜æœ‰ä¸‹ä¸€é¡µ
                        if not has_next_page and len(users) >= 50:
                            has_next_page = True
                            print(f"åŸºäºç”¨æˆ·æ•°é‡({len(users)})åˆ¤æ–­å¯èƒ½æœ‰ä¸‹ä¸€é¡µ")

                    except Exception as e:
                        print(f"æ£€æŸ¥ä¸‹ä¸€é¡µæ—¶å‡ºé”™: {e}")
                        # å¦‚æœå‡ºé”™ä¸”ç”¨æˆ·æ•°é‡è¾¾åˆ°50ï¼Œå‡è®¾æœ‰ä¸‹ä¸€é¡µ
                        if len(users) >= 50:
                            has_next_page = True

                    # ç»Ÿä¸€æ ¼å¼åŒ–æ•°æ®
                    users = [self._normalize_user_data(user, 'stargazer') for user in users]

                    print(f"æˆåŠŸæå–äº†ç¬¬{page}é¡µ {len(users)} ä¸ªstargazers")

                    return {
                        'data': users,
                        'has_next_page': has_next_page,
                        'current_page': page
                    }

                finally:
                    await browser.close()

        except Exception as e:
            print(f"çˆ¬å–stargazersç¬¬{page}é¡µæ—¶å‡ºé”™: {e}")
            return {
                'data': [],
                'has_next_page': False,
                'current_page': page
            }