import asyncio
import os
from typing import List, Dict, Any
from .base import BaseScraper
from .github.get_followers_list import GitHubFollowersListScraper
from .github.scrape_profiles import GitHubProfileScraper
from playwright.async_api import async_playwright
from datetime import datetime

class GitHubTwoStageScraper(BaseScraper):
    """GitHubä¸¤é˜¶æ®µçˆ¬å–å™¨ - æ•´åˆç‰ˆæœ¬"""

    def __init__(self):
        super().__init__()
        self.platform = "github"
        self.stage1_scraper = GitHubFollowersListScraper()
        self.stage2_scraper = GitHubProfileScraper()

    def get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´çš„ISOæ ¼å¼å­—ç¬¦ä¸²"""
        return datetime.now().isoformat()

    async def scrape_with_progress(self, url: str, max_pages: int = 5, max_users: int = 100):
        """
        æ‰§è¡Œå®Œæ•´çš„ä¸¤é˜¶æ®µçˆ¬å–æµç¨‹ï¼Œè¾¹çˆ¬è¾¹è¿”å›è¿›åº¦

        Args:
            url: GitHub URL
            max_pages: ç¬¬ä¸€é˜¶æ®µæœ€å¤§çˆ¬å–é¡µæ•°
            max_users: ç¬¬äºŒé˜¶æ®µæœ€å¤§å¤„ç†ç”¨æˆ·æ•°

        Yields:
            åŒ…å«è¿›åº¦ä¿¡æ¯çš„å­—å…¸
        """
        print(f"ğŸš€ å¼€å§‹GitHubä¸¤é˜¶æ®µæµå¼çˆ¬å–: {url}")

        # å‘é€å¼€å§‹æ¶ˆæ¯
        yield {
            'type': 'progress',
            'stage': 1,
            'message': 'åˆ†æURLå’Œå‡†å¤‡çˆ¬å–...',
            'progress': 0
        }

        # åˆ†æURLç±»å‹
        url_parts = url.rstrip('/').split('/')
        print(f"URLéƒ¨åˆ†: {url_parts}")

        stage1_csv = ""

        # æ ¹æ®max_usersè®¡ç®—éœ€è¦çš„é¡µæ•°ï¼ˆGitHubæ¯é¡µå¤§çº¦50ä¸ªç”¨æˆ·ï¼‰
        calculated_pages = max(1, min(max_pages, (max_users + 49) // 50))
        print(f"æ ¹æ®max_users={max_users}ï¼Œè®¡ç®—éœ€è¦çˆ¬å– {calculated_pages} é¡µ")

        yield {
            'type': 'progress',
            'stage': 1,
            'message': f'å‡†å¤‡çˆ¬å– {calculated_pages} é¡µç”¨æˆ·åˆ—è¡¨...',
            'progress': 5
        }

        if len(url_parts) >= 5 and url_parts[3] and url_parts[4]:
            # ä»“åº“URL: https://github.com/owner/repo
            owner = url_parts[3]
            repo = url_parts[4]
            print(f"è¯†åˆ«ä¸ºä»“åº“é¡µé¢: {owner}/{repo}")

            yield {
                'type': 'progress',
                'stage': 1,
                'message': f'æ­£åœ¨çˆ¬å–ä»“åº“ {owner}/{repo} çš„stargazers...',
                'progress': 10
            }

            # ç¬¬ä¸€é˜¶æ®µï¼šè·å–stargazersåˆ—è¡¨
            stage1_csv = await self.stage1_scraper.scrape_stargazers_list(owner, repo, calculated_pages)

        elif len(url_parts) >= 4 and url_parts[3]:
            # ç”¨æˆ·URL: https://github.com/username
            username = url_parts[3]
            print(f"è¯†åˆ«ä¸ºç”¨æˆ·é¡µé¢: {username}")

            yield {
                'type': 'progress',
                'stage': 1,
                'message': f'æ­£åœ¨çˆ¬å–ç”¨æˆ· {username} çš„followers...',
                'progress': 10
            }

            # ç¬¬ä¸€é˜¶æ®µï¼šè·å–followersåˆ—è¡¨
            stage1_csv = await self.stage1_scraper.scrape_followers_list(username, calculated_pages)

        else:
            yield {
                'type': 'error',
                'message': 'æ— æ³•è¯†åˆ«URLç±»å‹'
            }
            return

        if not stage1_csv or not os.path.exists(stage1_csv):
            yield {
                'type': 'error',
                'message': 'ç¬¬ä¸€é˜¶æ®µå¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆç”¨æˆ·åˆ—è¡¨æ–‡ä»¶'
            }
            return

        yield {
            'type': 'progress',
            'stage': 1,
            'message': f'ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼Œç”Ÿæˆæ–‡ä»¶: {os.path.basename(stage1_csv)}',
            'progress': 50
        }

        # ç¬¬äºŒé˜¶æ®µï¼šè·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯
        yield {
            'type': 'progress',
            'stage': 2,
            'message': f'å¼€å§‹ç¬¬äºŒé˜¶æ®µï¼šè·å–æœ€å¤š {max_users} ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯...',
            'progress': 60
        }

        # ä½¿ç”¨å¸¦è¿›åº¦çš„ç¬¬äºŒé˜¶æ®µçˆ¬å–
        async for progress in self.stage2_scraper.scrape_profiles_from_csv_with_progress(
            stage1_csv,
            max_users=max_users,
            batch_size=5
        ):
            # è°ƒæ•´è¿›åº¦èŒƒå›´ 60-95%
            adjusted_progress = 60 + (progress.get('progress', 0) * 0.35)
            yield {
                'type': 'progress',
                'stage': 2,
                'message': progress.get('message', 'å¤„ç†ç”¨æˆ·è¯¦ç»†ä¿¡æ¯...'),
                'progress': min(95, adjusted_progress),
                'current_user': progress.get('current_user', ''),
                'processed_count': progress.get('processed_count', 0),
                'total_count': progress.get('total_count', 0)
            }

        # è¯»å–æœ€ç»ˆç»“æœ
        yield {
            'type': 'progress',
            'stage': 2,
            'message': 'è¯»å–æœ€ç»ˆç»“æœ...',
            'progress': 95
        }

        final_data = await self._read_enriched_data(stage1_csv.replace('_raw.csv', '_enriched.csv'))

        yield {
            'type': 'complete',
            'data': final_data,
            'total': len(final_data),
            'message': f'çˆ¬å–å®Œæˆï¼å…±è·å– {len(final_data)} ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯',
            'progress': 100,
            'platform': 'github'
        }

    async def scrape(self, url: str, max_pages: int = 5, max_users: int = 100) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œå®Œæ•´çš„ä¸¤é˜¶æ®µçˆ¬å–æµç¨‹

        Args:
            url: GitHub URL
            max_pages: ç¬¬ä¸€é˜¶æ®µæœ€å¤§çˆ¬å–é¡µæ•°
            max_users: ç¬¬äºŒé˜¶æ®µæœ€å¤§å¤„ç†ç”¨æˆ·æ•°

        Returns:
            åŒ…å«è¯¦ç»†ä¿¡æ¯çš„ç”¨æˆ·åˆ—è¡¨
        """
        print(f"ğŸš€ å¼€å§‹GitHubä¸¤é˜¶æ®µçˆ¬å–: {url}")

        # åˆ†æURLç±»å‹
        url_parts = url.rstrip('/').split('/')
        print(f"URLéƒ¨åˆ†: {url_parts}")

        stage1_csv = ""

        # æ ¹æ®max_usersè®¡ç®—éœ€è¦çš„é¡µæ•°ï¼ˆGitHubæ¯é¡µå¤§çº¦50ä¸ªç”¨æˆ·ï¼‰
        calculated_pages = max(1, min(max_pages, (max_users + 49) // 50))  # å‘ä¸Šå–æ•´ï¼Œä½†ä¸è¶…è¿‡max_pages
        print(f"æ ¹æ®max_users={max_users}ï¼Œè®¡ç®—éœ€è¦çˆ¬å– {calculated_pages} é¡µ")

        if len(url_parts) >= 5 and url_parts[3] and url_parts[4]:
            # RepositoriesURL: https://github.com/owner/repo
            owner = url_parts[3]
            repo = url_parts[4]
            print(f"è¯†åˆ«ä¸ºRepositoriesé¡µé¢: {owner}/{repo}")

            # ç¬¬ä¸€é˜¶æ®µï¼šè·å–stargazersåˆ—è¡¨
            stage1_csv = await self.stage1_scraper.scrape_stargazers_list(owner, repo, calculated_pages)

        elif len(url_parts) >= 4 and url_parts[3]:
            # ç”¨æˆ·URL: https://github.com/username
            username = url_parts[3]
            print(f"è¯†åˆ«ä¸ºç”¨æˆ·é¡µé¢: {username}")

            # ç¬¬ä¸€é˜¶æ®µï¼šè·å–followersåˆ—è¡¨
            stage1_csv = await self.stage1_scraper.scrape_followers_list(username, calculated_pages)

        else:
            print("æ— æ³•è¯†åˆ«URLç±»å‹")
            return []

        if not stage1_csv or not os.path.exists(stage1_csv):
            print("ç¬¬ä¸€é˜¶æ®µå¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆç”¨æˆ·åˆ—è¡¨æ–‡ä»¶")
            return []

        print(f"ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼Œç”Ÿæˆæ–‡ä»¶: {stage1_csv}")

        # ç¬¬äºŒé˜¶æ®µï¼šè·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯
        print("ğŸ” å¼€å§‹ç¬¬äºŒé˜¶æ®µï¼šè·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯...")
        stage2_csv = await self.stage2_scraper.scrape_profiles_from_csv(
            stage1_csv,
            max_users=max_users,
            batch_size=5  # å°æ‰¹æ¬¡å¤„ç†ï¼Œé¿å…è¿‡è½½
        )

        if not stage2_csv or not os.path.exists(stage2_csv):
            print("ç¬¬äºŒé˜¶æ®µå¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆè¯¦ç»†ä¿¡æ¯æ–‡ä»¶")
            return []

        print(f"ç¬¬äºŒé˜¶æ®µå®Œæˆï¼Œç”Ÿæˆæ–‡ä»¶: {stage2_csv}")

        # è¯»å–æœ€ç»ˆç»“æœ
        return await self._read_enriched_data(stage2_csv)

    async def _read_enriched_data(self, csv_file_path: str) -> List[Dict[str, Any]]:
        """è¯»å–è¯¦ç»†ä¿¡æ¯CSVæ–‡ä»¶å¹¶è¿”å›æ•°æ®"""
        import csv

        users = []

        try:
            with open(csv_file_path, 'r', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
                    user_data = {
                        'username': row.get('username', ''),
                        'display_name': row.get('display_name', ''),
                        'bio': row.get('bio', ''),
                        'avatar_url': row.get('avatar_url', ''),
                        'profile_url': row.get('profile_url', ''),
                        'platform': 'github',
                        'type': row.get('type', 'user'),
                        'follower_count': self._safe_int(row.get('follower_count', '0')),
                        'following_count': self._safe_int(row.get('following_count', '0')),
                        'company': row.get('company', ''),
                        'location': row.get('location', ''),
                        'website': row.get('website', ''),
                        'twitter': row.get('twitter', ''),
                        'public_repos': self._safe_int(row.get('public_repos', '0')),
                        'scraped_at': row.get('profile_scraped_at', self.get_current_time()),
                        'additional_info': f"Source: {row.get('source_user', '')}{row.get('source_repo', '')}, Page: {row.get('page_number', '')}"
                    }
                    users.append(user_data)

            print(f"æˆåŠŸè¯»å– {len(users)} ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯")
            return users

        except Exception as e:
            print(f"è¯»å–è¯¦ç»†ä¿¡æ¯æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return []

    def _safe_int(self, value: str) -> int:
        """å®‰å…¨è½¬æ¢å­—ç¬¦ä¸²ä¸ºæ•´æ•°"""
        try:
            return int(value) if value else 0
        except:
            return 0

    async def _get_user_details(self, username: str, page_obj) -> Dict:
        """è·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯"""
        try:
            # è®¿é—®ç”¨æˆ·ä¸»é¡µ
            user_url = f"https://github.com/{username}"
            await page_obj.goto(user_url, wait_until='networkidle', timeout=15000)

            # ç­‰å¾…é¡µé¢åŠ è½½
            await page_obj.wait_for_timeout(1000)

            # æå–ç”¨æˆ·ä¿¡æ¯
            user_info = {
                'username': username,
                'display_name': username,
                'bio': '',
                'avatar_url': f"https://github.com/{username}.png",
                'profile_url': user_url,
                'platform': 'github',
                'type': 'follower',
                'follower_count': 0,
                'following_count': 0,
                'company': '',
                'location': '',
                'website': '',
                'twitter': '',
                'email': '',
                'public_repos': 0,
                'scraped_at': datetime.now().isoformat()
            }

            # è·å–ç”¨æˆ·åå’Œæ˜¾ç¤ºå
            try:
                name_element = await page_obj.query_selector('h1.vcard-names .p-name')
                if name_element:
                    display_name = await name_element.text_content()
                    if display_name and display_name.strip():
                        user_info['display_name'] = display_name.strip()
            except:
                pass

            # è·å–bio
            try:
                bio_element = await page_obj.query_selector('.p-note .user-profile-bio')
                if bio_element:
                    bio = await bio_element.text_content()
                    if bio and bio.strip():
                        user_info['bio'] = bio.strip()
            except:
                pass

            # è·å–followerå’Œfollowingæ•°é‡ - ä½¿ç”¨åŸscrape_profiles.pyçš„æˆåŠŸæ–¹æ³•
            try:
                import re
                # è·å–.js-profile-editable-areaä¸‹çš„æ‰€æœ‰é“¾æ¥
                follower_links = await page_obj.query_selector_all('.js-profile-editable-area a')
                for link in follower_links:
                    href = await link.get_attribute('href')
                    text = await link.text_content()
                    if href and text:
                        text = text.strip()
                        if 'followers' in href:
                            numbers = re.findall(r'\d+', text.replace(',', ''))
                            if numbers:
                                user_info['follower_count'] = int(numbers[0])
                                print(f"ç”¨æˆ· {username} followers: {numbers[0]}")
                        elif 'following' in href:
                            numbers = re.findall(r'\d+', text.replace(',', ''))
                            if numbers:
                                user_info['following_count'] = int(numbers[0])
                                print(f"ç”¨æˆ· {username} following: {numbers[0]}")

                # å¦‚æœä¸Šé¢çš„æ–¹æ³•æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•å¤‡ç”¨é€‰æ‹©å™¨
                if user_info['follower_count'] == 0:
                    # è°ƒè¯•ï¼šè¾“å‡ºé¡µé¢ä¸Šæ‰€æœ‰åŒ…å«followersçš„é“¾æ¥
                    try:
                        all_links = await page_obj.query_selector_all('a')
                        for link in all_links:
                            href = await link.get_attribute('href')
                            text = await link.text_content()
                            if href and 'followers' in href:
                                print(f"æ‰¾åˆ°followersé“¾æ¥: href={href}, text='{text}'")
                                # å°è¯•ä»é“¾æ¥æ–‡æœ¬ä¸­æå–æ•°å­—
                                if text:
                                    numbers = re.findall(r'\d+', text.replace(',', ''))
                                    if numbers:
                                        user_info['follower_count'] = int(numbers[0])
                                        print(f"å¤‡ç”¨æ–¹æ³•è·å–åˆ°ç”¨æˆ· {username} followers: {numbers[0]}")
                                        break
                            if href and 'following' in href:
                                print(f"æ‰¾åˆ°followingé“¾æ¥: href={href}, text='{text}'")
                                if text:
                                    numbers = re.findall(r'\d+', text.replace(',', ''))
                                    if numbers:
                                        user_info['following_count'] = int(numbers[0])
                                        print(f"å¤‡ç”¨æ–¹æ³•è·å–åˆ°ç”¨æˆ· {username} following: {numbers[0]}")
                    except:
                        pass

            except Exception as e:
                print(f"è·å– {username} å…³æ³¨æ•°æ®å¤±è´¥: {e}")
                pass

            # è·å–å…¬å¸ä¿¡æ¯
            try:
                company_selectors = [
                    '[data-test-selector="profile-company"] .p-org',
                    '.vcard-detail[itemprop="worksFor"] .p-org',
                    '.vcard-detail .p-org',
                    '.js-profile-editable-area [data-test-selector="profile-company"]'
                ]

                for selector in company_selectors:
                    company_element = await page_obj.query_selector(selector)
                    if company_element:
                        company = await company_element.text_content()
                        if company and company.strip():
                            user_info['company'] = company.strip()
                            print(f"User {username} company: {company.strip()}")
                            break
            except Exception as e:
                print(f"Failed to get user {username} company info: {e}")
                pass

            # è·å–ä½ç½®ä¿¡æ¯
            try:
                location_selectors = [
                    '[data-test-selector="profile-location"] .p-label',
                    '.vcard-detail[itemprop="homeLocation"] .p-label',
                    '.vcard-detail .p-label',
                    '.js-profile-editable-area [data-test-selector="profile-location"]'
                ]

                for selector in location_selectors:
                    location_element = await page_obj.query_selector(selector)
                    if location_element:
                        location = await location_element.text_content()
                        if location and location.strip():
                            user_info['location'] = location.strip()
                            print(f"User {username} location: {location.strip()}")
                            break
            except Exception as e:
                print(f"Failed to get user {username} location info: {e}")
                pass

            # è·å–é‚®ç®±ä¿¡æ¯ï¼Œåªä¿ç•™ itemprop="email" aria-label æ–¹å¼
            try:
                itemprop_email = await page_obj.query_selector('li[itemprop="email"]')
                if itemprop_email:
                    aria_label = await itemprop_email.get_attribute('aria-label')
                    if aria_label and 'Email:' in aria_label:
                        # æå– "Email: xxx@xxx.com" ä¸­çš„é‚®ç®±éƒ¨åˆ†
                        email_match = aria_label.split('Email:', 1)
                        if len(email_match) > 1:
                            email = email_match[1].strip()
                            if '@' in email and '.' in email:
                                user_info['email'] = email
                                print(f"User {username} email (from itemprop): {email}")
            except Exception as e:
                print(f"Failed to get user {username} email info: {e}")
                pass

            # è·å–ç½‘ç«™
            try:
                website_element = await page_obj.query_selector('[data-test-selector="profile-website"] .Link--primary')
                if website_element:
                    website = await website_element.get_attribute('href')
                    if website and website.strip():
                        user_info['website'] = website.strip()
            except:
                pass

            # è·å–å…¬å¼€Repositoriesæ•°é‡
            try:
                repos_element = await page_obj.query_selector('a[href$="?tab=repositories"] .Counter')
                if repos_element:
                    repos_text = await repos_element.text_content()
                    if repos_text:
                        repos_count = self._parse_count(repos_text.strip())
                        user_info['public_repos'] = repos_count
            except:
                pass

            return user_info

        except Exception as e:
            print(f"è·å–ç”¨æˆ· {username} è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
            # è¿”å›åŸºæœ¬ä¿¡æ¯
            return {
                'username': username,
                'display_name': username,
                'bio': '',
                'avatar_url': f"https://github.com/{username}.png",
                'profile_url': f"https://github.com/{username}",
                'platform': 'github',
                'type': 'follower',
                'follower_count': 0,
                'following_count': 0,
                'company': '',
                'location': '',
                'website': '',
                'twitter': '',
                'email': '',
                'public_repos': 0,
                'scraped_at': datetime.now().isoformat()
            }

    def _parse_count(self, count_str: str) -> int:
        """è§£æGitHubçš„æ•°é‡æ˜¾ç¤ºï¼ˆæ”¯æŒk, mç­‰å•ä½ï¼‰"""
        try:
            count_str = count_str.lower().replace(',', '')
            if 'k' in count_str:
                return int(float(count_str.replace('k', '')) * 1000)
            elif 'm' in count_str:
                return int(float(count_str.replace('m', '')) * 1000000)
            else:
                return int(count_str)
        except:
            return 0

    def _parse_url(self, url: str) -> tuple:
        """è§£æGitHub URLï¼Œç¡®å®šçˆ¬å–ç±»å‹"""
        try:
            # ç§»é™¤åè®®å’ŒåŸŸåï¼Œè·å–è·¯å¾„éƒ¨åˆ†
            if '://' in url:
                path_part = url.split('://', 1)[1]
                if '/' in path_part:
                    path = path_part.split('/', 1)[1]
                else:
                    path = ''
            else:
                path = url.strip('/')

            # åˆ†å‰²è·¯å¾„
            parts = [p for p in path.split('/') if p]
            print(f"è§£æURLè·¯å¾„éƒ¨åˆ†: {parts}")

            if not parts:
                raise ValueError("URLè·¯å¾„ä¸ºç©º")

            # æ£€æŸ¥æ˜¯å¦åŒ…å«tabå‚æ•°
            if '?' in url and 'tab=followers' in url:
                return "followers", parts[0] if parts else "", ""
            elif '?' in url and 'tab=stargazers' in url:
                return "stargazers", parts[0] if parts else "", ""
            elif len(parts) >= 2 and parts[1] == 'stargazers':
                # https://github.com/owner/repo/stargazers
                return "stargazers", parts[0], parts[1] if len(parts) > 1 else ""
            elif len(parts) >= 2:
                # https://github.com/owner/repo
                return "repo", parts[0], parts[1]
            elif len(parts) == 1:
                # https://github.com/username
                return "user", parts[0], ""
            else:
                raise ValueError(f"æ— æ³•è§£æçš„URLæ ¼å¼: {url}")

        except Exception as e:
            print(f"è§£æURLå¤±è´¥: {e}")
            raise ValueError(f"URLè§£æé”™è¯¯: {e}")

    async def scrape_page(self, url: str, page: int = 1) -> Dict:
        """åˆ†é¡µçˆ¬å–æ–¹æ³•"""
        try:
            print(f"GitHubåˆ†é¡µçˆ¬å–å™¨æ”¶åˆ°URL: {url}, é¡µç : {page}")

            # è§£æURLç¡®å®šçˆ¬å–ç±»å‹
            scrape_type, target_user, target_repo = self._parse_url(url)

            if scrape_type == "followers":
                print(f"è¯†åˆ«ä¸ºfollowersé¡µé¢ï¼Œç¬¬{page}é¡µ")
                return await self._scrape_followers_page(url, page)
            elif scrape_type == "stargazers":
                print(f"è¯†åˆ«ä¸ºstargazersé¡µé¢ï¼Œç¬¬{page}é¡µ")
                return await self._scrape_stargazers_page(url, target_user, target_repo, page)
            elif scrape_type == "user":
                print(f"è¯†åˆ«ä¸ºç”¨æˆ·é¡µé¢: {target_user}ï¼Œç¬¬{page}é¡µ")
                # é»˜è®¤çˆ¬å–ç”¨æˆ·çš„followers
                followers_url = f"https://github.com/{target_user}?tab=followers"
                return await self._scrape_followers_page(followers_url, page)
            elif scrape_type == "repo":
                print(f"è¯†åˆ«ä¸ºRepositoriesé¡µé¢: {target_user}/{target_repo}ï¼Œç¬¬{page}é¡µ")
                # é»˜è®¤çˆ¬å–Repositoriesçš„stargazers
                stargazers_url = f"https://github.com/{target_user}/{target_repo}/stargazers"
                return await self._scrape_stargazers_page(stargazers_url, target_user, target_repo, page)
            else:
                raise ValueError(f"æ— æ³•è¯†åˆ«çš„URLç±»å‹: {url}")

        except Exception as e:
            print(f"GitHubåˆ†é¡µçˆ¬å–å¤±è´¥: {e}")
            raise e

    async def _scrape_followers_page(self, url: str, page: int) -> Dict:
        """åˆ†é¡µçˆ¬å–followers"""
        try:
            print(f"å¼€å§‹çˆ¬å–å…³æ³¨è€…é¡µé¢ç¬¬{page}é¡µ: {url}")

            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context()
                page_obj = await context.new_page()

                try:
                    # æ„å»ºåˆ†é¡µURL
                    if '?' in url:
                        page_url = f"{url}&page={page}"
                    else:
                        page_url = f"{url}?page={page}"

                    print(f"è®¿é—®åˆ†é¡µURL: {page_url}")
                    await page_obj.goto(page_url, wait_until='networkidle', timeout=30000)

                    # ç­‰å¾…ç”¨æˆ·åˆ—è¡¨åŠ è½½
                    await page_obj.wait_for_selector('a[data-hovercard-type="user"]', timeout=10000)

                    # è·å–ç”¨æˆ·é“¾æ¥
                    user_links = await page_obj.query_selector_all('a[data-hovercard-type="user"]')
                    print(f"æ‰¾åˆ° {len(user_links)} ä¸ªç”¨æˆ·é“¾æ¥å…ƒç´ ")

                    # æå–ç”¨æˆ·ååˆ—è¡¨ï¼Œä½¿ç”¨setå»é‡
                    usernames = []
                    seen_usernames = set()
                    for link in user_links:
                        try:
                            href = await link.get_attribute('href')
                            if href and href.startswith('/'):
                                username = href.strip('/')
                                # å»é‡ï¼šå¦‚æœç”¨æˆ·åå·²ç»å­˜åœ¨ï¼Œè·³è¿‡
                                if username and username not in seen_usernames:
                                    seen_usernames.add(username)
                                    usernames.append(username)

                                    # é™åˆ¶æ¯é¡µæœ€å¤š50ä¸ªç”¨æˆ·
                                    if len(usernames) >= 50:
                                        break
                        except Exception as e:
                            print(f"æå–ç”¨æˆ·åå¤±è´¥: {e}")
                            continue

                    print(f"å¼€å§‹è·å– {len(usernames)} ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯...")

                    # è·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯
                    users = []
                    for i, username in enumerate(usernames):
                        try:
                            print(f"æ­£åœ¨è·å–ç”¨æˆ· {i+1}/{len(usernames)}: {username}")
                            user_info = await self._get_user_details(username, page_obj)
                            user_info['type'] = 'follower'
                            users.append(user_info)
                        except Exception as e:
                            print(f"è·å–ç”¨æˆ· {username} è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
                            # æ·»åŠ åŸºæœ¬ä¿¡æ¯
                            users.append({
                                'username': username,
                                'display_name': username,
                                'bio': '',
                                'avatar_url': f"https://github.com/{username}.png",
                                'profile_url': f"https://github.com/{username}",
                                'platform': 'github',
                                'type': 'follower',
                                'follower_count': 0,
                                'following_count': 0,
                                'company': '',
                                'location': '',
                                'website': '',
                                'twitter': '',
                                'email': '',
                                'public_repos': 0,
                                'scraped_at': datetime.now().isoformat()
                            })

                    # æŒ‰followeræ•°é‡æ’åºï¼ˆé™åºï¼‰
                    users.sort(key=lambda x: x['follower_count'], reverse=True)
                    print(f"ç”¨æˆ·æŒ‰followeræ•°é‡æ’åºå®Œæˆï¼Œæœ€é«˜: {users[0]['follower_count'] if users else 0}")

                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ - ä½¿ç”¨å¤šç§ç­–ç•¥
                    has_next_page = False
                    try:
                        # GitHubå¯èƒ½ä½¿ç”¨ä¸åŒçš„åˆ†é¡µé€‰æ‹©å™¨
                        selectors_to_check = [
                            '.pagination a[rel="next"]',
                            '.paginate-container .next_page',
                            '.paginate-container a[rel="next"]',
                            'a[aria-label="Next"]',
                            '.BtnGroup a[rel="next"]',
                            '.pagination .next_page:not(.disabled)',
                            '.paginate-container .next_page:not(.disabled)'
                        ]

                        for selector in selectors_to_check:
                            next_button = await page_obj.query_selector(selector)
                            if next_button:
                                # æ£€æŸ¥æŒ‰é’®æ˜¯å¦è¢«ç¦ç”¨
                                is_disabled = await next_button.get_attribute('aria-disabled')
                                class_name = await next_button.get_attribute('class') or ''
                                if is_disabled != 'true' and 'disabled' not in class_name:
                                    has_next_page = True
                                    print(f"æ‰¾åˆ°æœ‰æ•ˆçš„ä¸‹ä¸€é¡µæŒ‰é’®: {selector}")
                                    break

                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œæ£€æŸ¥å½“å‰é¡µé¢çš„ç”¨æˆ·æ•°é‡
                        # å¦‚æœæ­£å¥½æ˜¯50ä¸ªç”¨æˆ·ï¼Œå¾ˆå¯èƒ½è¿˜æœ‰ä¸‹ä¸€é¡µ
                        if not has_next_page and len(users) >= 50:
                            has_next_page = True
                            print(f"åŸºäºç”¨æˆ·æ•°é‡({len(users)})åˆ¤æ–­å¯èƒ½æœ‰ä¸‹ä¸€é¡µ")

                    except Exception as e:
                        print(f"æ£€æŸ¥ä¸‹ä¸€é¡µæ—¶å‡ºé”™: {e}")
                        # å¦‚æœå‡ºé”™ä¸”ç”¨æˆ·æ•°é‡è¾¾åˆ°50ï¼Œå‡è®¾æœ‰ä¸‹ä¸€é¡µ
                        if len(users) >= 50:
                            has_next_page = True

                    print(f"æˆåŠŸæå–äº†ç¬¬{page}é¡µ {len(users)} ä¸ªå…³æ³¨è€…")

                    return {
                        'data': users,
                        'has_next_page': has_next_page,
                        'current_page': page
                    }

                finally:
                    await browser.close()

        except Exception as e:
            print(f"çˆ¬å–followersç¬¬{page}é¡µæ—¶å‡ºé”™: {e}")
            return {
                'data': [],
                'has_next_page': False,
                'current_page': page
            }

    async def _scrape_stargazers_page(self, url: str, owner: str, repo: str, page: int) -> Dict:
        """åˆ†é¡µçˆ¬å–stargazers"""
        try:
            print(f"å¼€å§‹çˆ¬å–stargazersé¡µé¢ç¬¬{page}é¡µ: {url}")

            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context()
                page_obj = await context.new_page()

                try:
                    # æ„å»ºåˆ†é¡µURL
                    page_url = f"{url}?page={page}"

                    print(f"è®¿é—®åˆ†é¡µURL: {page_url}")
                    await page_obj.goto(page_url, wait_until='networkidle', timeout=30000)

                    # ç­‰å¾…ç”¨æˆ·åˆ—è¡¨åŠ è½½
                    await page_obj.wait_for_selector('a[data-hovercard-type="user"]', timeout=10000)

                    # è·å–ç”¨æˆ·é“¾æ¥
                    user_links = await page_obj.query_selector_all('a[data-hovercard-type="user"]')
                    print(f"æ‰¾åˆ° {len(user_links)} ä¸ªç”¨æˆ·é“¾æ¥å…ƒç´ ")

                    # æå–ç”¨æˆ·ååˆ—è¡¨ï¼Œä½¿ç”¨setå»é‡
                    usernames = []
                    seen_usernames = set()
                    for link in user_links:
                        try:
                            href = await link.get_attribute('href')
                            if href and href.startswith('/'):
                                username = href.strip('/')
                                # å»é‡ï¼šå¦‚æœç”¨æˆ·åå·²ç»å­˜åœ¨ï¼Œè·³è¿‡
                                if username and username not in seen_usernames:
                                    seen_usernames.add(username)
                                    usernames.append(username)

                                    # é™åˆ¶æ¯é¡µæœ€å¤š50ä¸ªç”¨æˆ·
                                    if len(usernames) >= 50:
                                        break
                        except Exception as e:
                            print(f"æå–ç”¨æˆ·åå¤±è´¥: {e}")
                            continue

                    print(f"å¼€å§‹è·å– {len(usernames)} ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯...")

                    # è·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯
                    users = []
                    for i, username in enumerate(usernames):
                        try:
                            print(f"æ­£åœ¨è·å–ç”¨æˆ· {i+1}/{len(usernames)}: {username}")
                            user_info = await self._get_user_details(username, page_obj)
                            user_info['type'] = 'stargazer'
                            users.append(user_info)
                        except Exception as e:
                            print(f"è·å–ç”¨æˆ· {username} è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
                            # æ·»åŠ åŸºæœ¬ä¿¡æ¯
                            users.append({
                                'username': username,
                                'display_name': username,
                                'bio': '',
                                'avatar_url': f"https://github.com/{username}.png",
                                'profile_url': f"https://github.com/{username}",
                                'platform': 'github',
                                'type': 'stargazer',
                                'follower_count': 0,
                                'following_count': 0,
                                'company': '',
                                'location': '',
                                'website': '',
                                'twitter': '',
                                'email': '',
                                'public_repos': 0,
                                'scraped_at': datetime.now().isoformat()
                            })

                    # æŒ‰followeræ•°é‡æ’åºï¼ˆé™åºï¼‰
                    users.sort(key=lambda x: x['follower_count'], reverse=True)
                    print(f"ç”¨æˆ·æŒ‰followeræ•°é‡æ’åºå®Œæˆï¼Œæœ€é«˜: {users[0]['follower_count'] if users else 0}")

                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ - ä½¿ç”¨å¤šç§ç­–ç•¥
                    has_next_page = False
                    try:
                        # GitHubå¯èƒ½ä½¿ç”¨ä¸åŒçš„åˆ†é¡µé€‰æ‹©å™¨
                        selectors_to_check = [
                            '.pagination a[rel="next"]',
                            '.paginate-container .next_page',
                            '.paginate-container a[rel="next"]',
                            'a[aria-label="Next"]',
                            '.BtnGroup a[rel="next"]',
                            '.pagination .next_page:not(.disabled)',
                            '.paginate-container .next_page:not(.disabled)'
                        ]

                        for selector in selectors_to_check:
                            next_button = await page_obj.query_selector(selector)
                            if next_button:
                                # æ£€æŸ¥æŒ‰é’®æ˜¯å¦è¢«ç¦ç”¨
                                is_disabled = await next_button.get_attribute('aria-disabled')
                                class_name = await next_button.get_attribute('class') or ''
                                if is_disabled != 'true' and 'disabled' not in class_name:
                                    has_next_page = True
                                    print(f"æ‰¾åˆ°æœ‰æ•ˆçš„ä¸‹ä¸€é¡µæŒ‰é’®: {selector}")
                                    break

                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œæ£€æŸ¥å½“å‰é¡µé¢çš„ç”¨æˆ·æ•°é‡
                        # å¦‚æœæ­£å¥½æ˜¯50ä¸ªç”¨æˆ·ï¼Œå¾ˆå¯èƒ½è¿˜æœ‰ä¸‹ä¸€é¡µ
                        if not has_next_page and len(users) >= 50:
                            has_next_page = True
                            print(f"åŸºäºç”¨æˆ·æ•°é‡({len(users)})åˆ¤æ–­å¯èƒ½æœ‰ä¸‹ä¸€é¡µ")

                    except Exception as e:
                        print(f"æ£€æŸ¥ä¸‹ä¸€é¡µæ—¶å‡ºé”™: {e}")
                        # å¦‚æœå‡ºé”™ä¸”ç”¨æˆ·æ•°é‡è¾¾åˆ°50ï¼Œå‡è®¾æœ‰ä¸‹ä¸€é¡µ
                        if len(users) >= 50:
                            has_next_page = True

                    print(f"æˆåŠŸæå–äº†ç¬¬{page}é¡µ {len(users)} ä¸ªstargazers")

                    return {
                        'data': users,
                        'has_next_page': has_next_page,
                        'current_page': page
                    }

                finally:
                    await browser.close()

        except Exception as e:
            print(f"çˆ¬å–stargazersç¬¬{page}é¡µæ—¶å‡ºé”™: {e}")
            return {
                'data': [],
                'has_next_page': False,
                'current_page': page
            }

# æµ‹è¯•å‡½æ•°
async def test_two_stage_scraper():
    """æµ‹è¯•ä¸¤é˜¶æ®µçˆ¬å–å™¨"""
    scraper = GitHubTwoStageScraper()

    # æµ‹è¯•ç”¨æˆ·followers
    print("=== æµ‹è¯•ç”¨æˆ·followersçˆ¬å– ===")
    users = await scraper.scrape("https://github.com/connor4312", max_pages=2, max_users=20)
    print(f"è·å–åˆ° {len(users)} ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯")

    if users:
        print("\nå‰3ä¸ªç”¨æˆ·è¯¦ç»†ä¿¡æ¯:")
        for i, user in enumerate(users[:3]):
            print(f"\nç”¨æˆ· {i+1}:")
            for key, value in user.items():
                if key in ['username', 'display_name', 'bio', 'company', 'location', 'follower_count', 'following_count']:
                    print(f"  {key}: {value}")

if __name__ == "__main__":
    asyncio.run(test_two_stage_scraper())