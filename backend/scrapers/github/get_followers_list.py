import asyncio
import csv
import os
from datetime import datetime
from typing import List, Dict, Any
from playwright.async_api import async_playwright

class GitHubFollowersListScraper:
    """GitHubç¬¬ä¸€é˜¶æ®µï¼šæ‰¹é‡è·å–followers/stargazersç”¨æˆ·ååˆ—è¡¨ï¼ˆæ”¯æŒåˆ†é¡µï¼‰"""
    
    def __init__(self):
        self.data_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'data')
        os.makedirs(self.data_dir, exist_ok=True)
    
    async def scrape_followers_list(self, username: str, max_pages: int = 10) -> str:
        """
        çˆ¬å–ç”¨æˆ·çš„followersåˆ—è¡¨
        
        Args:
            username: GitHubç”¨æˆ·å
            max_pages: æœ€å¤§çˆ¬å–é¡µæ•°
            
        Returns:
            CSVæ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸš€ ç¬¬ä¸€é˜¶æ®µï¼šå¼€å§‹çˆ¬å– {username} çš„followersåˆ—è¡¨...")
        
        playwright = await async_playwright().start()
        browser = await playwright.chromium.launch(headless=True)
        page = await browser.new_page()
        
        # è®¾ç½®ç”¨æˆ·ä»£ç†
        await page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        followers = []
        
        try:
            for page_num in range(1, max_pages + 1):
                # GitHub followersåˆ†é¡µURLæ ¼å¼
                url = f"https://github.com/{username}?page={page_num}&tab=followers"
                print(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page_num} é¡µ: {url}")
                
                await page.goto(url, wait_until="networkidle", timeout=30000)
                await asyncio.sleep(2)
                
                # è·å–å½“å‰é¡µé¢çš„ç”¨æˆ·é“¾æ¥
                user_links = await page.query_selector_all('a[data-hovercard-type="user"]')
                
                if not user_links:
                    print(f"ç¬¬ {page_num} é¡µæ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·é“¾æ¥ï¼Œåœæ­¢çˆ¬å–")
                    break
                
                page_followers = []
                seen_usernames = set()
                
                for link in user_links:
                    try:
                        href = await link.get_attribute('href')
                        if href and href.startswith('/'):
                            follower_username = href.strip('/')
                            if follower_username and follower_username not in seen_usernames:
                                seen_usernames.add(follower_username)
                                page_followers.append({
                                    'username': follower_username,
                                    'profile_url': f'https://github.com/{follower_username}',
                                    'type': 'follower',
                                    'source_user': username,
                                    'page_number': page_num,
                                    'scraped_at': datetime.now().isoformat()
                                })
                    except Exception as e:
                        print(f"å¤„ç†ç”¨æˆ·é“¾æ¥æ—¶å‡ºé”™: {e}")
                        continue
                
                print(f"ç¬¬ {page_num} é¡µè·å–åˆ° {len(page_followers)} ä¸ªfollowers")
                followers.extend(page_followers)
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
                next_selectors = [
                    '.pagination a:last-child',  # GitHubçš„ä¸‹ä¸€é¡µæŒ‰é’®
                    'a[href*="page=' + str(page_num + 1) + '"]',  # åŒ…å«ä¸‹ä¸€é¡µé¡µç çš„é“¾æ¥
                    '.pagination a[rel="next"]'  # æ ‡å‡†çš„nexté“¾æ¥
                ]
                
                has_next_page = False
                for selector in next_selectors:
                    try:
                        next_button = await page.query_selector(selector)
                        if next_button:
                            button_text = await next_button.text_content()
                            button_href = await next_button.get_attribute('href')
                            print(f"æ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®: text='{button_text}', href='{button_href}'")
                            
                            # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯ä¸‹ä¸€é¡µé“¾æ¥
                            if (button_text and 'next' in button_text.lower()) or \
                               (button_href and f'page={page_num + 1}' in button_href):
                                has_next_page = True
                                break
                    except Exception as e:
                        print(f"æ£€æŸ¥é€‰æ‹©å™¨ {selector} æ—¶å‡ºé”™: {e}")
                        continue
                
                if not has_next_page:
                    print(f"æ²¡æœ‰ä¸‹ä¸€é¡µï¼Œæ€»å…±çˆ¬å–äº† {page_num} é¡µ")
                    break
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(1)
            
            # ä¿å­˜åˆ°CSVæ–‡ä»¶
            csv_file = await self._save_to_csv(followers, f"{username}_followers_raw.csv")
            print(f"âœ… ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼æ€»å…±è·å– {len(followers)} ä¸ªfollowersï¼Œä¿å­˜åˆ°: {csv_file}")
            
            return csv_file
            
        except Exception as e:
            print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return ""
        finally:
            await browser.close()
            await playwright.stop()
    
    async def scrape_stargazers_list(self, owner: str, repo: str, max_pages: int = 10) -> str:
        """
        çˆ¬å–Repositoriesçš„stargazersåˆ—è¡¨
        
        Args:
            owner: Repositoriesæ‰€æœ‰è€…
            repo: Repositorieså
            max_pages: æœ€å¤§çˆ¬å–é¡µæ•°
            
        Returns:
            CSVæ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸš€ ç¬¬ä¸€é˜¶æ®µï¼šå¼€å§‹çˆ¬å– {owner}/{repo} çš„stargazersåˆ—è¡¨...")
        
        playwright = await async_playwright().start()
        browser = await playwright.chromium.launch(headless=True)
        page = await browser.new_page()
        
        # è®¾ç½®ç”¨æˆ·ä»£ç†
        await page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        stargazers = []
        
        try:
            for page_num in range(1, max_pages + 1):
                # GitHub stargazersåˆ†é¡µURLæ ¼å¼
                url = f"https://github.com/{owner}/{repo}/stargazers?page={page_num}"
                print(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page_num} é¡µ: {url}")
                
                await page.goto(url, wait_until="networkidle", timeout=30000)
                await asyncio.sleep(2)
                
                # è·å–å½“å‰é¡µé¢çš„ç”¨æˆ·é“¾æ¥
                user_links = await page.query_selector_all('a[data-hovercard-type="user"]')
                
                if not user_links:
                    print(f"ç¬¬ {page_num} é¡µæ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·é“¾æ¥ï¼Œåœæ­¢çˆ¬å–")
                    break
                
                page_stargazers = []
                seen_usernames = set()
                
                for link in user_links:
                    try:
                        href = await link.get_attribute('href')
                        if href and href.startswith('/'):
                            stargazer_username = href.strip('/')
                            if stargazer_username and stargazer_username not in seen_usernames:
                                seen_usernames.add(stargazer_username)
                                page_stargazers.append({
                                    'username': stargazer_username,
                                    'profile_url': f'https://github.com/{stargazer_username}',
                                    'type': 'stargazer',
                                    'source_repo': f'{owner}/{repo}',
                                    'page_number': page_num,
                                    'scraped_at': datetime.now().isoformat()
                                })
                    except Exception as e:
                        print(f"å¤„ç†ç”¨æˆ·é“¾æ¥æ—¶å‡ºé”™: {e}")
                        continue
                
                print(f"ç¬¬ {page_num} é¡µè·å–åˆ° {len(page_stargazers)} ä¸ªstargazers")
                stargazers.extend(page_stargazers)
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
                next_selectors = [
                    '.pagination a:last-child',  # GitHubçš„ä¸‹ä¸€é¡µæŒ‰é’®
                    'a[href*="page=' + str(page_num + 1) + '"]',  # åŒ…å«ä¸‹ä¸€é¡µé¡µç çš„é“¾æ¥
                    '.pagination a[rel="next"]'  # æ ‡å‡†çš„nexté“¾æ¥
                ]
                
                has_next_page = False
                for selector in next_selectors:
                    try:
                        next_button = await page.query_selector(selector)
                        if next_button:
                            button_text = await next_button.text_content()
                            button_href = await next_button.get_attribute('href')
                            print(f"æ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®: text='{button_text}', href='{button_href}'")
                            
                            # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯ä¸‹ä¸€é¡µé“¾æ¥
                            if (button_text and 'next' in button_text.lower()) or \
                               (button_href and f'page={page_num + 1}' in button_href):
                                has_next_page = True
                                break
                    except Exception as e:
                        print(f"æ£€æŸ¥é€‰æ‹©å™¨ {selector} æ—¶å‡ºé”™: {e}")
                        continue
                
                if not has_next_page:
                    print(f"æ²¡æœ‰ä¸‹ä¸€é¡µï¼Œæ€»å…±çˆ¬å–äº† {page_num} é¡µ")
                    break
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(1)
            
            # ä¿å­˜åˆ°CSVæ–‡ä»¶
            csv_file = await self._save_to_csv(stargazers, f"{owner}_{repo}_stargazers_raw.csv")
            print(f"âœ… ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼æ€»å…±è·å– {len(stargazers)} ä¸ªstargazersï¼Œä¿å­˜åˆ°: {csv_file}")
            
            return csv_file
            
        except Exception as e:
            print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return ""
        finally:
            await browser.close()
            await playwright.stop()
    
    async def _save_to_csv(self, data: List[Dict[str, Any]], filename: str) -> str:
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        if not data:
            return ""
        
        csv_path = os.path.join(self.data_dir, filename)
        
        # å®šä¹‰CSVå­—æ®µ
        fieldnames = ['username', 'profile_url', 'type', 'source_user', 'source_repo', 'page_number', 'scraped_at']
        
        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for item in data:
                # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
                row = {field: item.get(field, '') for field in fieldnames}
                writer.writerow(row)
        
        return csv_path

# æµ‹è¯•å‡½æ•°
async def main():
    scraper = GitHubFollowersListScraper()
    
    # æµ‹è¯•çˆ¬å–followers
    csv_file = await scraper.scrape_followers_list("connor4312", max_pages=3)
    print(f"ç»“æœæ–‡ä»¶: {csv_file}")

if __name__ == "__main__":
    asyncio.run(main()) 