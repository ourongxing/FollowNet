import asyncio
import csv
import os
from datetime import datetime
from typing import List, Dict, Any
from playwright.async_api import async_playwright
import re

class GitHubProfileScraper:
    """GitHubç¬¬äºŒé˜¶æ®µï¼šè·å–ç”¨æˆ·è¯¦ç»†èµ„æ–™ä¿¡æ¯"""

    def __init__(self):
        self.data_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'data')
        os.makedirs(self.data_dir, exist_ok=True)

    async def scrape_profiles_from_csv_with_progress(self, csv_file_path: str, max_users: int = 100, batch_size: int = 5):
        """
        ä»CSVæ–‡ä»¶è¯»å–ç”¨æˆ·åˆ—è¡¨ï¼Œè·å–æ¯ä¸ªç”¨æˆ·çš„è¯¦ç»†èµ„æ–™ï¼Œå¸¦è¿›åº¦æŠ¥å‘Š

        Args:
            csv_file_path: ç¬¬ä¸€é˜¶æ®µç”Ÿæˆçš„CSVæ–‡ä»¶è·¯å¾„
            max_users: æœ€å¤§å¤„ç†ç”¨æˆ·æ•°
            batch_size: æ‰¹æ¬¡å¤§å°

        Yields:
            åŒ…å«è¿›åº¦ä¿¡æ¯çš„å­—å…¸
        """
        print(f"ğŸ” ç¬¬äºŒé˜¶æ®µï¼šå¼€å§‹ä» {csv_file_path} è·å–ç”¨æˆ·è¯¦ç»†èµ„æ–™...")

        # è¯»å–ç¬¬ä¸€é˜¶æ®µçš„ç”¨æˆ·åˆ—è¡¨
        usernames = await self._read_usernames_from_csv(csv_file_path)

        if not usernames:
            yield {
                'type': 'error',
                'message': 'æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·ååˆ—è¡¨'
            }
            return

        # é™åˆ¶å¤„ç†æ•°é‡
        usernames = usernames[:max_users]
        total_users = len(usernames)

        yield {
            'type': 'progress',
            'message': f'å°†å¤„ç† {total_users} ä¸ªç”¨æˆ·',
            'progress': 0,
            'total_count': total_users,
            'processed_count': 0
        }

        playwright = await async_playwright().start()
        browser = await playwright.chromium.launch(headless=True)
        page = await browser.new_page()

        # è®¾ç½®ç”¨æˆ·ä»£ç†
        await page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })

        enriched_users = []
        processed_count = 0

        try:
            # åˆ†æ‰¹å¤„ç†ç”¨æˆ·
            for i in range(0, len(usernames), batch_size):
                batch = usernames[i:i + batch_size]
                batch_num = i // batch_size + 1

                yield {
                    'type': 'progress',
                    'message': f'å¤„ç†æ‰¹æ¬¡ {batch_num}: {len(batch)} ä¸ªç”¨æˆ·',
                    'progress': (processed_count / total_users) * 100,
                    'total_count': total_users,
                    'processed_count': processed_count
                }

                for username_data in batch:
                    username = username_data['username']
                    try:
                        yield {
                            'type': 'progress',
                            'message': f'æ­£åœ¨è·å–ç”¨æˆ·èµ„æ–™: {username}',
                            'progress': (processed_count / total_users) * 100,
                            'current_user': username,
                            'total_count': total_users,
                            'processed_count': processed_count
                        }

                        user_details = await self._get_user_details(username, page, username_data)
                        if user_details:
                            enriched_users.append(user_details)
                            processed_count += 1

                            yield {
                                'type': 'user_completed',
                                'message': f'âœ… æˆåŠŸè·å– {username} çš„èµ„æ–™',
                                'progress': (processed_count / total_users) * 100,
                                'current_user': username,
                                'total_count': total_users,
                                'processed_count': processed_count,
                                'user_data': user_details
                            }
                        else:
                            processed_count += 1
                            yield {
                                'type': 'user_failed',
                                'message': f'âŒ è·å– {username} çš„èµ„æ–™å¤±è´¥',
                                'progress': (processed_count / total_users) * 100,
                                'current_user': username,
                                'total_count': total_users,
                                'processed_count': processed_count
                            }
                    except Exception as e:
                        processed_count += 1
                        yield {
                            'type': 'user_error',
                            'message': f'è·å– {username} èµ„æ–™æ—¶å‡ºé”™: {e}',
                            'progress': (processed_count / total_users) * 100,
                            'current_user': username,
                            'total_count': total_users,
                            'processed_count': processed_count
                        }
                        continue

                # æ‰¹æ¬¡é—´æš‚åœ
                if i + batch_size < len(usernames):
                    yield {
                        'type': 'progress',
                        'message': 'æ‰¹æ¬¡é—´æš‚åœ...',
                        'progress': (processed_count / total_users) * 100,
                        'total_count': total_users,
                        'processed_count': processed_count
                    }
                    await asyncio.sleep(2)

            # ä¿å­˜è¯¦ç»†èµ„æ–™åˆ°æ–°çš„CSVæ–‡ä»¶
            yield {
                'type': 'progress',
                'message': f'ä¿å­˜ {len(enriched_users)} ä¸ªç”¨æˆ·çš„è¯¦ç»†èµ„æ–™...',
                'progress': 95,
                'total_count': total_users,
                'processed_count': processed_count
            }

            output_file = await self._save_enriched_csv(enriched_users, csv_file_path)

            yield {
                'type': 'complete',
                'message': f'âœ… ç¬¬äºŒé˜¶æ®µå®Œæˆï¼è·å–äº† {len(enriched_users)} ä¸ªç”¨æˆ·çš„è¯¦ç»†èµ„æ–™',
                'progress': 100,
                'total_count': total_users,
                'processed_count': processed_count,
                'output_file': output_file
            }

        except Exception as e:
            yield {
                'type': 'error',
                'message': f'ç¬¬äºŒé˜¶æ®µå¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}'
            }
        finally:
            await browser.close()
            await playwright.stop()

    async def scrape_profiles_from_csv(self, csv_file_path: str, max_users: int = 100, batch_size: int = 5) -> str:
        """
        ä»CSVæ–‡ä»¶è¯»å–ç”¨æˆ·åˆ—è¡¨ï¼Œè·å–æ¯ä¸ªç”¨æˆ·çš„è¯¦ç»†èµ„æ–™

        Args:
            csv_file_path: ç¬¬ä¸€é˜¶æ®µç”Ÿæˆçš„CSVæ–‡ä»¶è·¯å¾„
            max_users: æœ€å¤§å¤„ç†ç”¨æˆ·æ•°
            batch_size: æ‰¹æ¬¡å¤§å°

        Returns:
            åŒ…å«è¯¦ç»†èµ„æ–™çš„CSVæ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸ” ç¬¬äºŒé˜¶æ®µï¼šå¼€å§‹ä» {csv_file_path} è·å–ç”¨æˆ·è¯¦ç»†èµ„æ–™...")

        # è¯»å–ç¬¬ä¸€é˜¶æ®µçš„ç”¨æˆ·åˆ—è¡¨
        usernames = await self._read_usernames_from_csv(csv_file_path)

        if not usernames:
            print("æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·ååˆ—è¡¨")
            return ""

        # é™åˆ¶å¤„ç†æ•°é‡
        usernames = usernames[:max_users]
        print(f"å°†å¤„ç† {len(usernames)} ä¸ªç”¨æˆ·")

        playwright = await async_playwright().start()
        browser = await playwright.chromium.launch(headless=True)
        page = await browser.new_page()

        # è®¾ç½®ç”¨æˆ·ä»£ç†
        await page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })

        enriched_users = []

        try:
            # åˆ†æ‰¹å¤„ç†ç”¨æˆ·
            for i in range(0, len(usernames), batch_size):
                batch = usernames[i:i + batch_size]
                print(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} ä¸ªç”¨æˆ·")

                for username_data in batch:
                    username = username_data['username']
                    try:
                        print(f"æ­£åœ¨è·å–ç”¨æˆ·èµ„æ–™: {username}")
                        user_details = await self._get_user_details(username, page, username_data)
                        if user_details:
                            enriched_users.append(user_details)
                            print(f"âœ… æˆåŠŸè·å– {username} çš„èµ„æ–™")
                        else:
                            print(f"âŒ è·å– {username} çš„èµ„æ–™å¤±è´¥")
                    except Exception as e:
                        print(f"è·å– {username} èµ„æ–™æ—¶å‡ºé”™: {e}")
                        continue

                # æ‰¹æ¬¡é—´æš‚åœ
                if i + batch_size < len(usernames):
                    print("æ‰¹æ¬¡é—´æš‚åœ...")
                    await asyncio.sleep(2)

            # ä¿å­˜è¯¦ç»†èµ„æ–™åˆ°æ–°çš„CSVæ–‡ä»¶
            output_file = await self._save_enriched_csv(enriched_users, csv_file_path)
            print(f"âœ… ç¬¬äºŒé˜¶æ®µå®Œæˆï¼è·å–äº† {len(enriched_users)} ä¸ªç”¨æˆ·çš„è¯¦ç»†èµ„æ–™")

            return output_file

        except Exception as e:
            print(f"ç¬¬äºŒé˜¶æ®µå¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return ""
        finally:
            await browser.close()
            await playwright.stop()

    async def _read_usernames_from_csv(self, csv_file_path: str) -> List[Dict[str, Any]]:
        """ä»CSVæ–‡ä»¶è¯»å–ç”¨æˆ·ååˆ—è¡¨"""
        usernames = []

        try:
            with open(csv_file_path, 'r', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    if row.get('username'):
                        usernames.append({
                            'username': row['username'],
                            'type': row.get('type', 'user'),
                            'source_user': row.get('source_user', ''),
                            'source_repo': row.get('source_repo', ''),
                            'page_number': row.get('page_number', ''),
                            'scraped_at': row.get('scraped_at', '')
                        })

            print(f"ä»CSVæ–‡ä»¶è¯»å–åˆ° {len(usernames)} ä¸ªç”¨æˆ·å")
            return usernames

        except Exception as e:
            print(f"è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return []

    async def _get_user_details(self, username: str, page_obj, original_data: Dict) -> Dict:
        """è·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯"""
        try:
            # è®¿é—®ç”¨æˆ·ä¸»é¡µ
            user_url = f"https://github.com/{username}"
            await page_obj.goto(user_url, wait_until='networkidle', timeout=15000)

            # ç­‰å¾…é¡µé¢åŠ è½½
            await asyncio.sleep(1)

            # åˆå§‹åŒ–ç”¨æˆ·ä¿¡æ¯ï¼Œä¿ç•™ç¬¬ä¸€é˜¶æ®µçš„æ•°æ®
            user_info = {
                'username': username,
                'display_name': username,
                'bio': '',
                'avatar_url': f"https://github.com/{username}.png",
                'profile_url': user_url,
                'platform': 'github',
                'type': original_data.get('type', 'user'),
                'source_user': original_data.get('source_user', ''),
                'source_repo': original_data.get('source_repo', ''),
                'page_number': original_data.get('page_number', ''),
                'follower_count': 0,
                'following_count': 0,
                'company': '',
                'location': '',
                'website': '',
                'twitter': '',
                'email': '',
                'public_repos': 0,
                'scraped_at': original_data.get('scraped_at', ''),
                'profile_scraped_at': datetime.now().isoformat()
            }

            # è·å–æ˜¾ç¤ºå
            try:
                name_selectors = [
                    'h1.vcard-names .p-name',
                    '.vcard-fullname',
                    '[data-testid="profile-name"]',
                    '.js-profile-editable-names .p-name'
                ]

                for selector in name_selectors:
                    name_element = await page_obj.query_selector(selector)
                    if name_element:
                        display_name = await name_element.text_content()
                        if display_name and display_name.strip():
                            user_info['display_name'] = display_name.strip()
                            break
            except Exception as e:
                print(f"è·å–æ˜¾ç¤ºåæ—¶å‡ºé”™: {e}")

            # è·å–bio
            try:
                bio_selectors = [
                    '.p-note .user-profile-bio',
                    '[data-bio-text]',
                    '.js-user-profile-bio',
                    '.user-profile-bio'
                ]

                for selector in bio_selectors:
                    bio_element = await page_obj.query_selector(selector)
                    if bio_element:
                        bio = await bio_element.text_content()
                        if bio and bio.strip():
                            user_info['bio'] = bio.strip()
                            break
            except Exception as e:
                print(f"è·å–bioæ—¶å‡ºé”™: {e}")

            # è·å–å¤´åƒURL
            try:
                avatar_selectors = [
                    '.avatar-user',
                    '.avatar img',
                    '[data-testid="profile-avatar"] img'
                ]

                for selector in avatar_selectors:
                    avatar_element = await page_obj.query_selector(selector)
                    if avatar_element:
                        avatar_url = await avatar_element.get_attribute('src')
                        if avatar_url:
                            user_info['avatar_url'] = avatar_url
                            break
            except Exception as e:
                print(f"è·å–å¤´åƒæ—¶å‡ºé”™: {e}")

            # è·å–followerå’Œfollowingæ•°é‡
            try:
                # æŸ¥æ‰¾åŒ…å«followersçš„é“¾æ¥
                links = await page_obj.query_selector_all('a')
                for link in links:
                    href = await link.get_attribute('href')
                    text = await link.text_content()

                    if href and text:
                        text = text.strip()
                        if f'/{username}?tab=followers' in href or f'/{username}/followers' in href:
                            # æå–æ•°å­—
                            numbers = re.findall(r'[\d,]+', text)
                            if numbers:
                                count_str = numbers[0].replace(',', '')
                                try:
                                    user_info['follower_count'] = int(count_str)
                                except:
                                    pass
                        elif f'/{username}?tab=following' in href or f'/{username}/following' in href:
                            # æå–æ•°å­—
                            numbers = re.findall(r'[\d,]+', text)
                            if numbers:
                                count_str = numbers[0].replace(',', '')
                                try:
                                    user_info['following_count'] = int(count_str)
                                except:
                                    pass
            except Exception as e:
                print(f"è·å–å…³æ³¨æ•°æ®æ—¶å‡ºé”™: {e}")

            # è·å–å…¬å…±ä»“åº“æ•°é‡
            try:
                repo_links = await page_obj.query_selector_all('a')
                for link in repo_links:
                    href = await link.get_attribute('href')
                    text = await link.text_content()

                    if href and text and '?tab=repositories' in href:
                        numbers = re.findall(r'[\d,]+', text)
                        if numbers:
                            count_str = numbers[0].replace(',', '')
                            try:
                                user_info['public_repos'] = int(count_str)
                                break
                            except:
                                pass
            except Exception as e:
                print(f"è·å–ä»“åº“æ•°é‡æ—¶å‡ºé”™: {e}")

            # è·å–å…¶ä»–èµ„æ–™ä¿¡æ¯
            try:
                # å…¬å¸ã€ä½ç½®ã€ç½‘ç«™ç­‰ä¿¡æ¯é€šå¸¸åœ¨vcard-detailsä¸­
                detail_items = await page_obj.query_selector_all('.vcard-details li, .vcard-detail')

                for item in detail_items:
                    text = await item.text_content()
                    if text:
                        text = text.strip()
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä½ç½®ä¿¡æ¯
                        if any(keyword in text.lower() for keyword in ['location', 'ä½ç½®', 'based in']):
                            user_info['location'] = text
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…¬å¸ä¿¡æ¯
                        elif any(keyword in text.lower() for keyword in ['company', 'å…¬å¸', 'work', 'org']):
                            user_info['company'] = text
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç½‘ç«™é“¾æ¥
                        elif 'http' in text.lower():
                            user_info['website'] = text
            except Exception as e:
                print(f"è·å–è¯¦ç»†èµ„æ–™æ—¶å‡ºé”™: {e}")

            return user_info

        except Exception as e:
            print(f"è·å–ç”¨æˆ· {username} è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return None

    async def _save_enriched_csv(self, users: List[Dict[str, Any]], original_csv_path: str) -> str:
        """ä¿å­˜å¢å¼ºåçš„ç”¨æˆ·æ•°æ®åˆ°CSVæ–‡ä»¶"""
        if not users:
            return ""

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        base_name = os.path.basename(original_csv_path)
        name_parts = base_name.split('_raw.csv')
        if len(name_parts) == 2:
            output_filename = f"{name_parts[0]}_enriched.csv"
        else:
            output_filename = f"enriched_{base_name}"

        output_path = os.path.join(self.data_dir, output_filename)

        # å®šä¹‰CSVå­—æ®µ
        fieldnames = [
            'username', 'display_name', 'bio', 'avatar_url', 'profile_url',
            'platform', 'type', 'source_user', 'source_repo', 'page_number',
            'follower_count', 'following_count', 'public_repos',
            'company', 'location', 'website', 'twitter', 'email',
            'scraped_at', 'profile_scraped_at'
        ]

        with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()

            for user in users:
                # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
                row = {field: user.get(field, '') for field in fieldnames}
                writer.writerow(row)

        print(f"è¯¦ç»†èµ„æ–™å·²ä¿å­˜åˆ°: {output_path}")
        return output_path

# æµ‹è¯•å‡½æ•°
async def main():
    scraper = GitHubProfileScraper()

    # å‡è®¾å·²æœ‰ç¬¬ä¸€é˜¶æ®µçš„CSVæ–‡ä»¶
    csv_file = "/path/to/connor4312_followers_raw.csv"
    if os.path.exists(csv_file):
        result_file = await scraper.scrape_profiles_from_csv(csv_file, max_users=10)
        print(f"ç»“æœæ–‡ä»¶: {result_file}")
    else:
        print("è¯·å…ˆè¿è¡Œç¬¬ä¸€é˜¶æ®µè·å–ç”¨æˆ·åˆ—è¡¨")

if __name__ == "__main__":
    asyncio.run(main())